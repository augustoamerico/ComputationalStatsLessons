[
["index.html", "Computational Stats 1 Lesson 1 1.1 Start by creating a vector 1.2 Now a Matrix! 1.3 DataFrames 1.4 Reading a Tab Separated File 1.5 Generating data 1.6 Getting insights 1.7 Lists 1.8 Functions", " Computational Stats Tiago dos Santos 2018-11-16 1 Lesson 1 x &lt;- 3+5 ls() ## [1] &quot;LatexOrOther&quot; &quot;datasetsDir&quot; &quot;fig_basePath&quot; &quot;x&quot; 1.1 Start by creating a vector y &lt;- c(2,5,9,8) y[1:3] ## [1] 2 5 9 y[c(1,3)] ## [1] 2 9 1.1.0.1 Get the elements 1,2,3 from the vector y[1:3] ## [1] 2 5 9 1.1.0.2 Get the elements 1,3 from the vector y[c(1,3)] ## [1] 2 9 1.1.0.3 Get an array from 0 to 1, with a 0.001 step y &lt;- 1:1000/1000 y &lt;- seq(0,1,0.001) 1.1.0.4 Which values are lower than 0.008? isValueLowerThan &lt;- y &lt; 0.008 y[isValueLowerThan] ## [1] 0.000 0.001 0.002 0.003 0.004 0.005 0.006 0.007 idxs &lt;- which(y&lt;0.08) y[idxs] ## [1] 0.000 0.001 0.002 0.003 0.004 0.005 0.006 0.007 0.008 0.009 0.010 ## [12] 0.011 0.012 0.013 0.014 0.015 0.016 0.017 0.018 0.019 0.020 0.021 ## [23] 0.022 0.023 0.024 0.025 0.026 0.027 0.028 0.029 0.030 0.031 0.032 ## [34] 0.033 0.034 0.035 0.036 0.037 0.038 0.039 0.040 0.041 0.042 0.043 ## [45] 0.044 0.045 0.046 0.047 0.048 0.049 0.050 0.051 0.052 0.053 0.054 ## [56] 0.055 0.056 0.057 0.058 0.059 0.060 0.061 0.062 0.063 0.064 0.065 ## [67] 0.066 0.067 0.068 0.069 0.070 0.071 0.072 0.073 0.074 0.075 0.076 ## [78] 0.077 0.078 0.079 1.1.0.5 Creating objects by repetition colors &lt;- c(&quot;amarelo&quot;,&quot;verde&quot;,&quot;vermelho&quot;,&quot;azul&quot;) rep(colors, 5) ## [1] &quot;amarelo&quot; &quot;verde&quot; &quot;vermelho&quot; &quot;azul&quot; &quot;amarelo&quot; &quot;verde&quot; ## [7] &quot;vermelho&quot; &quot;azul&quot; &quot;amarelo&quot; &quot;verde&quot; &quot;vermelho&quot; &quot;azul&quot; ## [13] &quot;amarelo&quot; &quot;verde&quot; &quot;vermelho&quot; &quot;azul&quot; &quot;amarelo&quot; &quot;verde&quot; ## [19] &quot;vermelho&quot; &quot;azul&quot; print(&quot;===&quot;) ## [1] &quot;===&quot; rep(10,5) ## [1] 10 10 10 10 10 1.2 Now a Matrix! M &lt;- matrix(1:9, ncol=3) M ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 Transposing the Matrix t(M) ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 ## [3,] 7 8 9 Accessing the Matrix M[1,2] ## [1] 4 M[1,] ## [1] 1 4 7 M[,2] ## [1] 4 5 6 Matrix Operation M2 &lt;- t(M) M+M2 # valuewise add ## [,1] [,2] [,3] ## [1,] 2 6 10 ## [2,] 6 10 14 ## [3,] 10 14 18 M*M2 # valuewise multiplication ## [,1] [,2] [,3] ## [1,] 1 8 21 ## [2,] 8 25 48 ## [3,] 21 48 81 M%*%M2 # Matricial Multiplication ## [,1] [,2] [,3] ## [1,] 66 78 90 ## [2,] 78 93 108 ## [3,] 90 108 126 1.2.0.1 Joining Matrixes Matrix Operation cbind(M,M2) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 4 7 1 2 3 ## [2,] 2 5 8 4 5 6 ## [3,] 3 6 9 7 8 9 rbind(M,M2) ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 ## [4,] 1 2 3 ## [5,] 4 5 6 ## [6,] 7 8 9 1.2.0.2 Inverting a matrix #solve(M) # M must not be singular 1.3 DataFrames y &lt;- 1:10 y2 &lt;- 11:20 y3 &lt;- letters[1:10] d1 &lt;- data.frame(y,y2,y3) d1 ## y y2 y3 ## 1 1 11 a ## 2 2 12 b ## 3 3 13 c ## 4 4 14 d ## 5 5 15 e ## 6 6 16 f ## 7 7 17 g ## 8 8 18 h ## 9 9 19 i ## 10 10 20 j 1.4 Reading a Tab Separated File emp &lt;- read.table(file.path(datasetsDir,&quot;empresas.txt&quot;), header=F) knitr::kable(head(emp)) V1 V2 V3 V4 V5 Soflor 2 5 10 3 Florinha 3 10 22 7 Flora 5 30 55 18 Floflo 2 5 12 4 Fazflor 3 15 28 8 Comercflor 2 10 18 5 dim(emp) ## [1] 40 5 names(emp) &lt;- c(&quot;nome&quot;,&quot;n.socios&quot;,&quot;c.social&quot;,&quot;vmm&quot;,&quot;n.emp&quot;) knitr::kable(head(emp)) nome n.socios c.social vmm n.emp Soflor 2 5 10 3 Florinha 3 10 22 7 Flora 5 30 55 18 Floflo 2 5 12 4 Fazflor 3 15 28 8 Comercflor 2 10 18 5 emp$n.socios ## [1] 2 3 5 2 3 2 3 4 6 5 2 3 2 3 2 3 3 2 5 2 2 3 3 2 2 2 2 4 4 3 2 2 4 2 2 ## [36] 2 3 3 3 2 emp[,2] ## [1] 2 3 5 2 3 2 3 4 6 5 2 3 2 3 2 3 3 2 5 2 2 3 3 2 2 2 2 4 4 3 2 2 4 2 2 ## [36] 2 3 3 3 2 1.5 Generating data set.seed(5) emp$ant &lt;- round(rnorm(dim(emp)[1],10,1)) 1.6 Getting insights summary(emp) ## nome n.socios c.social vmm ## Alecrim : 1 Min. :2.00 Min. : 5.00 Min. : 5.00 ## Beijaflor : 1 1st Qu.:2.00 1st Qu.: 5.00 1st Qu.: 11.00 ## Caflor : 1 Median :3.00 Median :10.00 Median : 19.00 ## Comercflor: 1 Mean :2.85 Mean :11.72 Mean : 24.48 ## Cravinho : 1 3rd Qu.:3.00 3rd Qu.:15.00 3rd Qu.: 31.00 ## Cravo : 1 Max. :6.00 Max. :50.00 Max. :100.00 ## (Other) :34 ## n.emp ant ## Min. : 2.000 Min. : 8 ## 1st Qu.: 3.000 1st Qu.: 9 ## Median : 5.500 Median :10 ## Mean : 6.225 Mean :10 ## 3rd Qu.: 9.000 3rd Qu.:11 ## Max. :18.000 Max. :12 ## mean(emp$n.socios) ## [1] 2.85 sd(emp$n.socios) ## [1] 1.051251 tapply(emp$vmm, emp$n.emp, mean) # vmm mean by number of employes ## 2 3 4 5 6 7 ## 8.714286 10.875000 12.666667 18.000000 22.000000 23.000000 ## 8 9 10 11 14 15 ## 28.000000 32.250000 45.000000 61.000000 45.000000 100.000000 ## 16 18 ## 55.000000 55.000000 tapply(emp$vmm, emp$n.emp, sd) # vmm sd by number of employes ## 2 3 4 5 6 7 8 9 ## 2.627691 1.457738 1.154701 0.000000 1.414214 1.414214 NA 1.500000 ## 10 11 14 15 16 18 ## NA 1.414214 NA NA NA NA \\[ \\overline{X} = \\frac{1}{N}\\sum\\limits_{i=1}^{N}X_{i} \\] \\[\\begin{equation} \\label{eq:std} S^2 = \\frac{1}{N}\\sum\\limits_{i=1}^{N}(X_{i} - \\overline{X})^2 \\end{equation}\\] table(emp$n.emp) #first line are values, second line is frequency ## ## 2 3 4 5 6 7 8 9 10 11 14 15 16 18 ## 7 8 3 2 6 2 1 4 1 2 1 1 1 1 barplot(emp$n.emp) # each company is a bin in x label, y is the number of employees barplot(table(emp$n.emp), xlab=&quot;#Employees&quot;, ylab=&quot;Frequecy&quot;, col=&quot;pink&quot;) boxplot(emp$vmm,range=0,col=&quot;purple&quot;,horizontal=T,main=&quot;vmm&quot;) boxplot(emp$vmm ~ emp$n.emp,range=0,col=&quot;purple&quot;,horizontal=T,main=&quot;vmm&quot;) hist(emp$vmm) hist(emp$vmm, freq=F) lines(density(emp$vmm),col=2) par(mfrow=c(1,2)) hist(emp$vmm) hist(emp$vmm, freq=F) lines(density(emp$vmm),col=2) par(mfrow=c(1,1)) plot(emp$vmm,emp$n.socios,pch=16) plot(emp) 1.7 Lists uma.lista &lt;- list( um.vector=1:10, uma.palavra=&quot;olá&quot;, uma.matrix=M, outra.lista=list( a=&quot;flor&quot;, b=rep(3,5) ) ) uma.lista[&quot;um.vector&quot;] ## $um.vector ## [1] 1 2 3 4 5 6 7 8 9 10 uma.lista$um.vector ## [1] 1 2 3 4 5 6 7 8 9 10 uma.lista[1] ## $um.vector ## [1] 1 2 3 4 5 6 7 8 9 10 1.8 Functions desconto &lt;- function(price, discount=25){ #Discount is a number between 0 and 100 #calcula o desconto de um preço newPrice &lt;- price*(1-discount/100) discount &lt;- price - newPrice list( novo.preco=newPrice, desconto=discount) } desconto(1000,20) ## $novo.preco ## [1] 800 ## ## $desconto ## [1] 200 desconto(1000,25) ## $novo.preco ## [1] 750 ## ## $desconto ## [1] 250 This is how you function "],
["lessons-2.html", "2 Lessons 2 2.1 Random Variables and Vectors", " 2 Lessons 2 2.1 Random Variables and Vectors 2.1.1 Elements of probability A random variable \\(\\bold{X}\\) is a function that takes an event space and return a value: \\[ X: \\Omega \\rightarrow {\\rm I\\!R}\\] 2.1.2 Expected value "],
["lesson-3.html", "3 Lesson 3 3.1 Hypothesis Testing", " 3 Lesson 3 g &lt;- function(x){ exp(x^2) } #create sample from uniform distribution sample &lt;- runif(10000) sample.length &lt;- length(sample) mean(g(sample)) ## [1] 1.46525 3.0.1 Estimating pi g &lt;- function(x){ sqrt(1-x^2) } #create sample from uniform distribution sample &lt;- runif(100000000) mean(g(sample))*4 ## [1] 3.141693 gIndicatriz &lt;- function(x,y){ ifelse((x^2 + y^2) &lt;= 1, 1, 0) } sampleX &lt;- runif(1000000) sampleY &lt;- runif(1000000) mean(gIndicatriz(sampleX,sampleY))*4 ## [1] 3.141924 3.1 Hypothesis Testing A statistical hypothesis is some conjecture about the distribution of one or more random variables. For each hypothesis designated by null hypothesis and denoted by \\(H0\\), there is always an alternative hypothesis denoted by \\(H1\\). We start the test by believing that \\(H0\\) is true, and during the test we can discard that hypothesis only if the data points there. Moreover, we can see these hypothesis testing as: A statistical hypothesis is some statement about the parameters of one or more populations (parametric tests) or about the distribution of the population (non-parametric tests). The goal of a test is to use the information of a data sample to decide (reject or no reject) about a conjecture over unknown aspects of a given population. 3.1.1 Types of error while infering through hypothesis testing There are always some risk associated to statistical inference: *** Type 1 error ***: reject \\(H0\\) when \\(H0\\) is true (rejecting error, aka False Negative in ML nomenclature) *** Type 2 error ***: accept \\(H0\\) when \\(H0\\) is false (no rejecting error, aka False Positive in ML nomenclature) 3.1.2 Defining \\(\\alpha\\) to reduce a type of error \\[ \\alpha = P(Type1Err) = P(Rejecting H0 | H0 is true) \\] So, we define \\(\\alpha\\) as being the probability that we want for the Type 1 error - or how much are we willing to be prone to this type of error. Therefone, \\(\\alpha\\) is called significance level of the test (a test that is very prone to errors is not very significant, right?) In general, we assign a very small value to the probability of type I error (0.05 ou 0.01). On the other end of the error spectrum, we define \\(\\beta\\) as \\[ \\beta = P(Type2Err) = P(Accepting H0 | H0 is false) \\] where \\(1 - \\beta\\) is called power of the test. The insight here is that the lower the \\(\\beta\\), the more “power” this test have. 3.1.3 Procedure to make a test using \\(p-value\\) 3.1.3.1 Wait, what is p-value? (WIP) 3.1.4 Estimating test stats The hypothesis being tested is the following: We have a sample (the variable popSample below) of independent observations from a random variable that we know follows an exponencial distribution, with unknown parameter \\(\\lambda\\). We want to test if \\(\\lambda = 3\\). popSample &lt;- c(0.2,1.2,2.9,1.2,0.1,0.1,0.4,0.1,0.7,0.1,0.9,0.3,0.6,0.1,0.2,0.1,0.4,0.1,0.3,1.4) lambdaEstimator &lt;- function(sample){ 1/mean(sample) } parameter &lt;- 3 testStatsEstimator &lt;- function(sample,hypothesisLambda, estimatedLambda){ sampleMean &lt;- mean(sample) sampleLength &lt;- length(sample) return( ( 1/((sampleMean*hypothesisLambda)^sampleLength)) * (exp( sampleLength* ( hypothesisLambda*sampleMean -1) ) ) ) } tobs &lt;- testStatsEstimator(popSample,parameter,lambdaEstimator(popSample)) Here, we will do the following 1000 times: we get a random sample from an exponential with \\(\\lambda\\) = 3 we obtain the estimated test statistic for this sample By the end of this process, we will get 1000 values that represente possible values of the Test Statistic Function empiricDistTestStats &lt;- sapply(1:1000,function(idx){ sampleTest &lt;- rexp(length(popSample),parameter) testStatsEstimator(sampleTest,parameter,lambdaEstimator(sampleTest)) }) empiricDistTestStats &lt;- c(empiricDistTestStats,tobs) empiricDistTestStats.df &lt;- as.data.frame(empiricDistTestStats) names(empiricDistTestStats.df) &lt;- c(&quot;values&quot;) empiricFrequency &lt;- empiricDistTestStats.df %&gt;% dplyr::group_by(values) %&gt;% dplyr::summarise(n=n()) p_value_estimated &lt;- sum(empiricFrequency[empiricFrequency$values &gt;= tobs,]$n)/sum(empiricFrequency$n) a &lt;- list( text = paste0(&quot;P value estimated: &quot; , round(p_value_estimated,5)), x = tobs, y = 0.3, xref = &quot;x&quot;, yref = &quot;y&quot;, ax = 50 ) plotly::plot_ly( x = empiricDistTestStats , type=&quot;histogram&quot; , histnorm = &quot;probability&quot; , name = &quot;Empiric Frequency&quot;) %&gt;% plotly::add_segments( x = tobs, xend = tobs, y = 0, yend = 0.3, name = &quot;T obs&quot; ) %&gt;% plotly::layout(annotations=a) "],
["deliverables.html", "4 Deliverables", " 4 Deliverables "],
["trabalho-1.html", "5 Trabalho 1 5.1 Exerc?cio 1 5.2 Exercise 2", " 5 Trabalho 1 5.1 Exerc?cio 1 Considere que uma variável continua \\(X\\) com a seguinte função de densidade: \\[ f(x) = \\left\\{ \\begin{array}{ll} \\frac{4}{3}(x^3 + x) \\quad \\qquad 0 &lt; x &lt; 1\\\\ 0, \\qquad \\qquad \\qquad\\text{for all others } x \\text{ values}\\\\ \\end{array} \\right. \\] Agora considerando a variavel aliatória \\(Y = g(X)\\), em que \\(g(x) = log(x^2 + 4)\\). Estime o valor \\(P(1.3 &lt; Y &lt; 1.5)\\) usando o método de monte carlo e estime o valor do desvio padrão da estimativa \\[ P(1.3 &lt; Y &lt; 1.5) \\quad = \\quad P(1.3 &lt; g(x) &lt; 1.5) \\] \\[ D(g(x))=D(log(x^2+4))=[min(log(x^2+4),+ \\infty[ \\quad , \\quad min(log(x^2+4))=log(4)=1.386294&gt;1.3 \\] Onde \\(D(g(x))\\) é o dominio de \\(g(x)\\). Uma vez que \\(D(g(x))\\) está definido no intervalo \\([min(log(x^2+4),+ \\infty[\\) afimar o seguinte: \\[ P(1.3&lt;g(x)&lt;log(4))=0 \\quad \\implies \\quad P(1.3&lt;g(x)&lt;1.5)\\equiv P(log(4)&lt;g(X)&lt;1.5) \\] Que se pode desenvolver : \\[ P(log(4) &lt; log(x^2 + 4) &lt; 1.5) \\quad = \\quad P(4 &lt; x^2 + 4 &lt; e^{1.5}) \\quad = \\quad P(0 &lt; x^2 &lt; e^{1.5}-4) \\quad = \\quad P(0 &lt; x &lt; \\sqrt{e^{1.5}-4}) \\] Agora, sabemos que a probabilidade que queriamos calcular pode ser obtida através do seguinte integral: \\[ \\int_{0}^{\\sqrt{e^{1.5}-4}} \\frac{4}{3}(x^3 + x) dx \\] O qual puderá ser escrito com a seguinte mudança de variável: 5.1.1 Mudança de variável \\[ z(x)=xc \\quad ,\\quad z(\\sqrt{e^{1.5}-4})=1 \\quad \\implies c=\\frac{1}{\\sqrt{e^{1.5}-4}} \\] \\[ x=z\\sqrt{e^{1.5}-4} \\quad \\implies \\quad x&#39;=\\sqrt{e^{1.5}-4} \\] No que resulta no seguinte integral: \\[ \\int_{0}^{\\sqrt{e^{1.5}-4}} \\frac{4}{3}(x^3 + x)dx \\quad \\equiv \\quad \\frac{4}{3}\\int_{0}^{1} ((z\\sqrt{e^{1.5}-4})^3 + z\\sqrt{e^{1.5}-4}).\\sqrt{e^{1.5}-4} \\;dz \\equiv \\frac{4}{3}(e^{1.5}-4)\\int_{0}^{1} (z^3(e^{1.5}-4) + z).1 \\; dx \\] Onde pudemos usar o método de monte carlo para estimar \\[\\int_{0}^{1} (z^3(e^{1.5}-1) + z).1 \\; dx\\] , assumindo que \\(z\\) segue uma distribuição \\(U(0,1)\\). \\[ \\int_{0}^{1} (z^3(e^{1.5}-4) + z).1 \\; dx\\quad \\approx \\quad \\hat{\\theta} =\\frac{\\sum_{i=1}^{n}(z_i^3.(e^{1.5}-4)+z_i)}{n} \\] Calculo do estimador \\(\\hat{\\theta}\\) do valor esperado do integral anterior. int_func &lt;- function(z){ res=(z^3)*(exp(1.5)-4)+z } #z follows an uniform sample &lt;- runif(1000) int_est &lt;- mean(int_func(sample)) prob_value &lt;- (4/3)*(exp(1.5)-4)*int_est Assim sendo: \\(P(1.3&lt;x&lt;1.5)\\quad=\\quad\\) 0.3937988 5.1.2 Cálculo do desvio padrão do estimador da probabilidade \\[ var(P(1.3&lt;Y&lt;1.5)) \\quad = \\quad \\left(\\frac{4}{3}(e^{1.5}-4)\\right)^2.var(\\theta) \\] varEstimator &lt;- (1/(length(sample)^2))*sum(((4/3)*(exp(1.5)-4)*int_func(sample)-prob_value)^2) df &lt;- data.frame( probEstimated = prob_value, stdMC = sqrt(varEstimator) ) knitr::kable(df) probEstimated stdMC 0.3937988 0.0085771 5.2 Exercise 2 5.2.1 2.1 \\[E(e^{x+y})=E(e^{x}+e^{y})\\] Sendo: \\[E(X)=\\int_D \\mathrm{x.f(x)}\\,\\mathrm{d}x\\], onde \\(\\mathrm{X}\\) é uma variável aletória e \\(\\mathrm{f(x)}\\) a sua função densidade de probabilidade. Temos: \\[\\int_0^\\infty\\int_0^\\infty\\mathrm{e}^{x+y}.\\frac{2}{\\sqrt{2\\pi}}.\\mathrm{e}^{\\frac{-x^{2}}{2}}.\\frac{2}{\\sqrt{2\\pi}}.\\mathrm{e}^{\\frac{-y^{2}}{2}}\\,\\mathrm{d}x\\mathrm{d}y \\quad = \\quad \\frac{2}{\\pi}\\int_0^\\infty\\int_0^\\infty\\mathrm{e}^{x}.\\mathrm{e}^{\\frac{-x^{2}}{2}}.\\mathrm{e}^{y}.\\mathrm{e}^{\\frac{-y^{2}}{2}}\\,\\mathrm{d}x\\mathrm{d}y\\] Fazendo as seguintes mudanças de variável: \\[\\alpha=\\mathrm{e}^{-x}\\implies x=-log(\\alpha)\\] \\[\\beta=\\mathrm{e}^{-y}\\implies y=-log(\\beta)\\] Ficamos com os seguintes limites de integração para \\(\\alpha\\): \\[\\displaystyle\\lim_{x \\to \\infty}\\mathrm{e}^{-x} = 0\\] \\[\\displaystyle\\lim_{x \\to 0}\\mathrm{e}^{-x} = 1\\] e para \\(\\beta\\): \\[\\displaystyle\\lim_{x \\to \\infty} \\mathrm{e}^{-y} = 0\\] \\[\\displaystyle\\lim_{x \\to 0} \\mathrm{e}^{-y} = 1\\] Substituindo na equação, temos: \\[\\frac{2}{\\pi}\\int_1^0\\int_1^0\\mathrm{e}^{-log(\\alpha)}.\\mathrm{e}^{\\frac{-(-log^{2}(\\alpha))}{2}}.(-\\frac{1}{\\alpha}).\\mathrm{e}^{-log(\\beta)}.\\mathrm{e}^{\\frac{-(-log^{2}(\\beta))}{2}}.(-\\frac{1}{\\beta})\\mathrm{d}\\alpha\\mathrm{d}\\beta\\] \\[ =\\frac{2}{\\pi}.(-\\int_0^1\\mathrm{e}^{-log(\\alpha)}.\\mathrm{e}^{\\frac{-(-log^{2}(\\alpha))}{2}}.\\frac{1}{\\alpha}\\mathrm{d}\\alpha).(-\\int_0^1\\mathrm{e}^{-log(\\beta)}.\\mathrm{e}^{\\frac{-(-log^{2}(\\beta))}{2}}.\\frac{1}{\\beta}\\mathrm{d}\\beta)\\] \\[=\\frac{2}{\\pi}.\\int_0^1\\mathrm{e}^{-log(\\alpha)}.\\mathrm{e}^{\\frac{-log^{2}(\\alpha)}{2}}.\\frac{1}{\\alpha}\\mathrm{d}\\alpha\\int_0^1\\mathrm{e}^{-log(\\beta)}.\\mathrm{e}^{\\frac{-log^{2}(\\beta)}{2}}.\\frac{1}{\\beta}\\mathrm{d}\\beta\\] \\[=\\frac{2}{\\pi}.\\int_0^1\\mathrm{e}^{-log(\\alpha).(1+\\frac{1}{2}log(\\alpha))}.\\frac{1}{\\alpha}\\mathrm{d}\\alpha.\\int_0^1\\mathrm{e}^{-log(\\beta).(1+\\frac{1}{2}log(\\beta))}.\\frac{1}{\\beta}\\mathrm{d}\\beta\\] \\[=\\frac{2}{\\pi}.\\int_0^1\\mathrm{e}^{-log(\\alpha).(1+\\frac{1}{2}log(\\alpha))}.\\frac{1}{\\alpha}.1\\mathrm{d}\\alpha.\\int_0^1\\mathrm{e}^{-log(\\beta).(1+\\frac{1}{2}log(\\beta))}.\\frac{1}{\\beta}.1\\mathrm{d}\\beta\\] \\[=\\frac{2}{\\pi}.\\int_0^1\\mathrm{e}^{-log(\\alpha).(1+\\frac{1}{2}log(\\alpha))}.\\frac{1}{\\alpha}.\\frac{1}{1-0}\\mathrm{d}\\alpha.\\int_0^1\\mathrm{e}^{-log(\\beta).(1+\\frac{1}{2}log(\\beta))}.\\frac{1}{\\beta}.\\frac{1}{1-0}\\mathrm{d}\\beta\\] Sendo: \\[h_1(\\alpha)=\\mathrm{e}^{-log(\\alpha).(1+\\frac{1}{2}log(\\alpha))}.\\frac{1}{\\alpha}.\\frac{1}{1-0}\\] \\[g_1(\\alpha)=\\mathrm{e}^{-log(\\alpha).(1+\\frac{1}{2}log(\\alpha))}.\\frac{1}{\\alpha}\\] \\[f_1(\\alpha)=\\frac{1}{1-0}\\] \\[h_2(\\beta)=\\mathrm{e}^{-log(\\beta).(1+\\frac{1}{2}log(\\beta))}.\\frac{1}{\\beta}.\\frac{1}{1-0}.\\] \\[g_2(\\beta)=\\mathrm{e}^{-log(\\beta).(1+\\frac{1}{2}log(\\beta))}.\\frac{1}{\\beta}\\] \\[f_2(\\beta)=\\frac{1}{1-0}\\] Onde: \\[f_1(\\alpha),f_2(\\beta)\\sim\\mathcal{U}(1,0)\\] e, \\[f_1(\\alpha),f_2(\\beta)\\geq0\\] Estamos em condições de aplicar Monte Carlo: \\[ \\begin{cases} \\theta_1=\\int_Dh_1(\\alpha)\\mathrm{d}\\alpha=\\int_Dg_1(\\alpha).f_1(\\alpha)\\mathrm{d}\\alpha=E(g_1(X)) \\\\\\theta_2=\\int_Dh_2(\\beta)\\mathrm{d}\\beta=\\int_Dg_2(\\beta).f_2(\\beta)\\mathrm{d}\\beta=E(g_2(Y)) \\end{cases} \\] Se tivermos uma amostra aleatória \\(x_1,...,x_n\\) da variavél aleatória \\(X\\) com densidade \\(f\\), um estimador \\(\\theta\\) é: \\[ \\begin{cases} \\hat{\\theta_1}=\\sum\\limits_{i=1}^n \\frac{g_1(x_i)}{n} \\\\\\hat{\\theta_2}=\\sum\\limits_{i=1}^n \\frac{g_2(y_i)}{n} \\end{cases} \\] Finalmente: \\(\\hat{E(e^{x+y})}=\\hat{\\theta}=\\frac{2}{\\pi}.\\hat{\\theta_1.}\\hat{\\theta_2}\\) A variância de \\(\\hat\\theta\\) será: \\(v=Var(\\frac{2}{\\pi}.\\hat{\\theta_1.}\\hat{\\theta_2})=\\frac{4}{\\pi^2}.Var(\\hat{\\theta_1}).Var(\\hat{\\theta_2})\\) Aplicando o método de Monte Carlo, ficamos com: \\[ \\begin{cases} \\hat Var(\\hat{\\theta_1})=\\frac{1}{n^2}\\sum\\limits_{i=1}^n (g_1(x_i)-\\hat{\\theta_1})^2 \\\\\\hat Var(\\hat{\\theta_2})=\\frac{1}{n^2}\\sum\\limits_{i=1}^n (g_2(y_i)-\\hat{\\theta_2})^2 \\end{cases} \\] Substituindo na equação inicial: \\[ \\hat v=\\frac{4}{\\pi^2}.\\sum\\limits_{i=1}^n \\frac{(g_1(x_i)-\\theta)^2}{n}.\\sum\\limits_{i=1}^n \\frac{(g_2(y_i)-\\theta)^2}{n} \\] 5.2.2 Implementação do método: 5.2.2.1 Determinação do estimador e do estimador da variância set.seed(1) n&lt;-1000 u1&lt;-runif(n) u2&lt;-runif(n) g1&lt;-function(x){exp(-log(x)*(1+(1/2)*log(x)))*(1/x)} g2&lt;-function(y){exp(-log(y)*(1+(1/2)*log(y)))*(1/y)} teta&lt;-(2/pi)*mean(g1(u1))*mean(g2(u2)) teta1&lt;-mean(g1(u1)) teta2&lt;-mean(g2(u2)) v&lt;-(4/(pi^2))*(mean((g1(u1)-teta1)^2)/n)*(mean((g2(u2)-teta2)^2)/n) df &lt;- data.frame( probEstimated = teta, varianceMC = v ) knitr::kable(df) probEstimated varianceMC 7.874321 8.3e-06 5.2.3 2.2 Dado que: \\[ X, Y \\text{ random variables with p.d.f.:} \\quad \\] \\[ f(x) = \\frac{2}{sqrt(2\\pi)}e^{-\\frac{x^2}{2}} \\quad , 0 &lt; x &lt; +\\infty \\] precisamos de estimar o par?metro \\(\\theta\\) com o método de Monte Carlo utilizando uma variável que não seja Uniforme, onde o parâmetro \\(\\theta\\) é definido como: \\[ \\theta = E(e^{X+Y}) \\] 5.2.4 Trabalhar o estimador \\[ \\theta = E(e^{X+Y}) \\quad = E(e^X \\times e^Y) \\quad = E(e^X) \\times E(e^Y) \\] \\[ = \\int_{0}^{+\\infty} e^x \\frac{2}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}} dx \\times \\int_{0}^{+\\infty} e^y \\frac{2}{\\sqrt{2\\pi}} e^{-\\frac{y^2}{2}} dy \\] Este integral n?o as condições para o método de Monte Carlo ser aplicado, e portanto é necessário trabalhar o integral de maneira a que seja possível aplicar o método. 5.2.5 Mudança de variável Ao aplicar a seguinte mudança de variável: \\[ x = \\varphi(t) = \\sqrt{t} \\] \\[ t = \\varphi^{-1}(x) = x^2 \\] \\[ \\varphi&#39;(t) = (\\sqrt{t})&#39; = (t^\\frac{1}{2})&#39; = \\frac{1}{2}t^{-\\frac{1}{2}} \\] \\[ \\lim_{t \\to +\\infty} \\sqrt{t} = +\\infty \\] \\[ \\lim_{t \\to 0} \\sqrt{t} = 0 \\] podemos reorganizar o integral da seguinte maneira: \\[ x = \\varphi(t) = t_x \\quad, y = \\varphi(t) = t_y \\] \\[ \\int_{0}^{+\\infty} e^{\\sqrt{t_x}} \\frac{2}{\\sqrt{2\\pi}} e^{-\\frac{1}{2}t_x} \\frac{1}{2}t_x^{-\\frac{1}{2}} dt_x \\times \\int_{0}^{+\\infty} e^{\\sqrt{t_y}} \\frac{2}{\\sqrt{2\\pi}} e^{-\\frac{1}{2}t_y} \\frac{1}{2}t_y^{-\\frac{1}{2}} dt_y \\] Re-ordenando a equação, estamos agora em condições de aplicar o método de Monte Carlo uma vez que o integral é definido pela multiplicação de: uma p.d.f. \\(f(x)\\) conhecida uma outra função \\(g(x)\\) \\[ \\int_{0}^{+\\infty} e^{\\sqrt{t_x}} \\frac{2}{\\sqrt{2\\pi}} t_x^{-\\frac{1}{2}} \\frac{1}{2}e^{-\\frac{1}{2}t_x} dt_x \\times \\int_{0}^{+\\infty} e^{\\sqrt{t_y}} \\frac{2}{\\sqrt{2\\pi}} t_y^{-\\frac{1}{2}} \\frac{1}{2} e^{-\\frac{1}{2}t_y} dt_y \\] onde: \\[ g(x) = e^{\\sqrt{x}} \\frac{2}{\\sqrt{2\\pi}} x^{-\\frac{1}{2}} \\quad \\text{and} \\quad f(x) = \\frac{1}{2} e^{-\\frac{1}{2}x} \\] onde \\(f(x)\\) é a função distribuição densidade de uma variável Exponencial com \\(\\lambda = \\frac{1}{2}\\): \\[ X \\sim Exp( \\frac{1}{2}) \\] Portanto, é agora necessário gerar amostras aleatórias de \\(X \\sim Exp(\\frac{1}{2})\\) 5.2.6 Gerar uma variável com distribuição Exponencial utilizando o método da Transformação Inversa Comecemos com a função distribui??o cumulativa de uma variável Exponencial, que é definida por: \\[ F(X) = 1 - e^{-\\lambda x}, \\quad x \\in \\mathbb{R} \\] Tendo em consideração que o resultado de \\(F(X)\\) é um número real entre 0 e 1, e que: \\(F(X)\\) ? uma função monótona não decrescente \\(F(X)\\) ? uma função cont?nua ? sabido que \\(F(X)\\) ? invert?vel. 5.2.7 Implementação do método set.seed(1) lambda &lt;- 0.5 N &lt;- 1000 samples &lt;- runif(N) inverseExp &lt;- function(u, lambda){ -(1/lambda)*log(1-u) } values &lt;- inverseExp(samples, lambda) hist(values, breaks=100, freq = F) lines(density(rexp(1000,0.5))) g &lt;- function(x){ exp(sqrt(x))*(2/(sqrt(2*pi)))*x^(-1/2) } X &lt;- runif(N) Y &lt;- runif(N) EX &lt;- mean(g(inverseExp(X, lambda))) EY &lt;- mean(g(inverseExp(Y, lambda))) theta2 &lt;- EX*EY vEX &lt;- (1/(N^2))*sum((g(inverseExp(X, lambda))-EX)^2) vEY &lt;- (1/(N^2))*sum((g(inverseExp(Y, lambda))-EY)^2) vtheta &lt;-vEX*vEY df &lt;- data.frame( probEstimated = theta2, varianceMC = vtheta ) knitr::kable(df) probEstimated varianceMC 7.901322 2.4e-06 5.2.8 2.3 \\(\\hat{\\theta}\\) = \\(E(e^{x+y})\\) \\(var(\\hat{\\theta})= \\frac{4}{\\pi^2}.\\sum\\limits_{i=1}^n \\frac{(g_1(x_i)-\\hat{\\theta})^2}{n}.\\sum\\limits_{i=1}^n \\frac{(g_2(y_i)-\\hat{\\theta})^2}{n}\\) novo estimador: \\(\\hat{\\theta_c}\\quad=\\quad \\hat{\\theta}-\\beta.(c-\\mu)\\) \\(E(C)\\quad=\\quad\\mu\\) \\(var(\\hat{\\theta_c}) = var(\\hat{\\theta})+\\beta^2.var(C)-2\\beta cov(\\hat{\\theta},C)\\) Queremos minimazar a variância, minimizando a variável \\(\\beta\\): para tal derivamos \\(var(\\hat{\\theta_c})\\) em ordem a \\(\\beta\\) o que resulta na expressão: \\(var(\\hat{\\theta_c})&#39;=2\\beta var(C) - 2cov(\\hat{\\theta},C)\\) Com \\(var(\\hat{\\theta_c})&#39;=0\\) iremos obter os extremos. \\(\\beta=\\frac{-2cov(\\hat{\\theta},C)}{2var(C)}\\) Calculo auxiliares: \\[ b)\\quad \\quad \\quad var(\\hat{\\theta})=Var\\left(\\frac{2}{\\pi}.(\\hat{\\theta_1}.\\hat{\\theta_2})\\right)=\\frac{4}{\\pi^2}.Var(\\hat{\\theta_1}).Var(\\hat{\\theta_2}) \\] \\[ E(C)=E\\left(\\frac{1}{n}.\\sum_{i=0}^n u_i\\:v_i \\right)= \\frac{1}{n}\\sum (E(u).E(v))=E(u).E(v) =\\int_0^1 u \\: du.\\int_0^1 v \\:dv \\] \\[ var(C)= \\frac{1}{n}.\\left(\\int_0^1f_c(x)^2-E(C)^2)\\right) \\] \\[ Cov(\\hat{\\theta},C)= cov \\left(\\frac{1}{n}\\sum_{i=1}^n g(U_i,V_i),\\frac{1}{n}\\sum_{i=1}^n U_i,V_i \\right)= \\] \\[ = \\frac{1}{n^2}\\sum_{i=1}^n cov \\left( g(U_i,V_i), U_i,V_i \\right) -\\sum_{i=1}^n\\sum_{j=1}^ncov \\left( g(U_i,V_i), U_jV_j \\right)= \\] Como os indices \\(i \\neq j\\) então\\(g(U_i,V_i)\\) será independente de \\(U_i V_i\\) e a sua covariância será Zero. \\[ = \\frac{1}{n^2}\\sum_{i=1}^n cov \\left( g(U_i,V_i), U_i,V_i \\right)-0= \\] \\[ =\\frac{1}{n}(E(g(U,V))UV))-\\frac{\\theta}{4n} \\] \\[ E(g(U,V))UV)= \\int_0^{1}\\int_0^{1}uv.g(u,v)\\:du\\:dv \\] que será estimado em r pelo método de monte carlo kk=1 res_final &lt;- data.frame(integer(), double(), double(), double(), double()) n &lt;- 10 while(kk&lt;6) { if(kk!=1) { if(kk%%2!=0) n&lt;-n*2 else n&lt;-n*5 } set.seed(1) u1&lt;-runif(n) u2&lt;-runif(n) g1&lt;-function(x){exp(-log(x)*(1+(1/2)*log(x)))*(1/x)} g2&lt;-function(y){exp(-log(y)*(1+(1/2)*log(y)))*(1/y)} teta1&lt;-mean(g1(u1)) teta2&lt;-mean(g2(u2)) teta&lt;-(2/pi)*mean(g1(u1))*mean(g2(u2)) teta_var&lt;-(4/(pi^2))*((mean((g1(u1)-teta1)^2))/n)*((mean((g2(u2)-teta2)^2))/n) df &lt;- data.frame( probEstimated = teta, varianceMC = teta_var ) g1_b&lt;-function(x){exp(-log(x)*(1+(1/2)*log(x)))} g2_b&lt;-function(y){exp(-log(y)*(1+(1/2)*log(y)))} covar_tc&lt;-((1/n^2)*mean(g1_b(u1)*g2_b(u2)*u1*u2))-teta/(4*n^2) c_var&lt;-(7/(n*144)) beta&lt;-covar_tc/(c_var) tetac&lt;-(teta - (beta*((1/4) - mean(u1*u2)))) tetac_var&lt;-(teta_var+(beta^2)*c_var-2*beta*covar_tc) res&lt;-data.frame(n, teta, teta_var, tetac, tetac_var) res_final&lt;-rbind(res_final,res) kk&lt;-kk+1 } knitr::kable(res_final) n teta teta_var tetac tetac_var 10 7.114578 0.0728507 6.962646 0.0401382 50 7.444414 0.0024827 7.432106 0.0021721 100 7.604431 0.0006777 7.598763 0.0006353 500 7.839631 0.0000307 7.839400 0.0000303 1000 7.874321 0.0000083 7.874559 0.0000083 "],
["problem-1.html", "6 Problem 1 6.1 Normal variance MLE 6.2 Jackknife bias and variance 6.3 Kolmogorov-Smirnov Test", " 6 Problem 1 Consider the following sample: c(7.0,3.5,11.9,8.9,10.1,1.2,1.1,7.9,12.9,1.3,5.2,5.1,3.9,2.5,10.4,6.2,-3.9) 6.1 Normal variance MLE Assume that the given sample originated from a random variable with a normal distribution whose parameter \\(\\sigma^2\\) is unknown. Use the sample to determine the maximum-likelihood estimator of the parameter \\(\\sigma^2\\). Maximum normal distribution Likelihood estimation. Assuming that our sample follows a normal distribution as said on the exercise question, we can say that the probability density function follows this format \\[ f_X(x_j)=(2. \\pi. \\sigma_0^2).e^{-\\frac{1}{2}.\\frac{(xj-\\mu)^2}{\\sigma_0^2}} \\] And to optain the maximum likelihood estimation, we need to partially derivate the likelihood in order to the two parameters, \\(\\mu\\) and \\(\\sigma^2\\), and discover the zeros. \\[ L(\\mu,\\sigma^2,x_1,...,x_n)=\\prod_{j=1}^n f_X(x_j;\\mu,\\sigma^2) \\] \\[ =\\prod_{j=1}^n(2\\pi\\sigma^2)^{-1/2}.e^{-\\frac{1}{2}.\\frac{(xj-\\mu)^2}{\\sigma^2}} \\] \\[ =(2\\pi\\sigma^2)^{-n/2}.e^{-\\frac{1}{2\\sigma^2}\\sum_{j=1}^{n}(xj-\\mu)^2} \\] Likelihood function: \\[ l(\\mu,\\sigma^2;x_1,...,x_n)=-\\frac{n}{2}ln(2\\pi)-\\frac{n}{2}ln(\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum_{j=1}^n(x_j-\\mu)^2 \\] Derivates equals to zero \\[ \\frac{\\partial}{\\partial \\mu}l(\\mu,\\sigma^2;x_1,...,x_n)=0 \\] \\[ \\frac{\\partial}{\\partial \\mu}l(\\mu,\\sigma^2;x_1,...,x_n)=\\frac{\\partial}{\\partial \\mu}\\bigg(-\\frac{n}{2}ln(2\\pi)-\\frac{n}{2}ln(\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum_{j=1}^n(x_j-\\mu)^2\\bigg)=\\frac{1}{\\sigma^2}(\\sum_{j=1}^nx_j-n\\mu) \\] \\[ \\frac{1}{\\sigma^2}(\\sum_{j=1}^nx_j-n\\mu)=0 \\equiv\\bigg(\\sum_{j=1}^nx_j-n\\mu\\bigg)=0 \\] Then: \\[ \\hat{\\mu}=\\frac{1}{n}\\bigg(\\sum_{j=1}^nx_j\\bigg) \\] \\[ \\frac{\\partial}{\\partial \\sigma^2}l(\\mu,\\sigma^2;x_1,...,x_n)=0 \\] \\[ \\frac{\\partial}{\\partial \\sigma^2}l(\\mu,\\sigma^2;x_1,...,x_n)=\\frac{\\partial}{\\partial \\mu}\\bigg(-\\frac{n}{2}ln(2\\pi)-\\frac{n}{2}ln(\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum_{j=1}^n(x_j-\\mu)^2)\\bigg)=\\frac{1}{2\\sigma^2}\\bigg[\\frac{1}{\\sigma^2}\\sum_{j=1}^{n}(x_j-\\mu)^2-n\\bigg] \\] \\[ \\frac{1}{2\\sigma^2}\\bigg[\\frac{1}{\\sigma^2}\\sum_{j=1}^{n}(x_j-\\mu)^2-n\\bigg]=0\\equiv\\frac{1}{\\sigma^2}\\sum_{j=1}^{n}(x_j-\\mu)^2-n=0 \\] Then: \\[ \\widehat{\\sigma^2}=\\frac{1}{n}\\sum_{j=1}^{n}(x_j-\\mu)^2 \\] sample&lt;-c(7.0,3.5,11.9,8.9,10.1,1.2,1.1,7.9,12.9,1.3,5.2,5.1,3.9,2.5,10.4,6.2,-3.9) norm.maximLikelihoodEst.var &lt;- function(sample){ n &lt;- length(sample) sample.mean &lt;- mean(sample) sum((sample - sample.mean)^2)/n } norm.maximLikelihoodEst.var(sample) ## [1] 18.84941 6.2 Jackknife bias and variance Determine the Jackknife bias and variance of the estimator obtained in the item (a), and discuss the quality of the estimator. We want to estimate the bias and variability of the estimator, \\(\\theta=t(F)\\). Since a good estimator not sensible to the sample is often of interest to know it faces minor pertubations in \\(F\\). We can define: \\[ L_t(y;F)=\\lim_{\\epsilon \\rightarrow 0} \\frac{t[(1-\\epsilon)F+\\epsilon H_y]-t(F)}{\\epsilon} \\] \\[ H_y= \\left\\{ \\begin{array}{ll} 1, u \\leq y \\\\ 0, u \\geq y \\end{array} \\right.\\tag{2.0} \\] as the influence fuction of \\(t\\) at \\(F\\). Using \\((2.0)\\) with the empirical function we get the empirical influence function: \\[ I(y)=L_t(y;\\hat F)\\tag{2.1} \\] Applying an extension of Taylor’s Theorem to \\(t(\\hat F)\\): \\[t(\\hat F) \\simeq t(F)+\\frac{1}{n}\\sum_{j=1}^n{I_j}\\tag{2.2}\\] So: \\[\\theta-\\hat{\\theta}=-\\frac{1}{n} \\sum_{j=1}^n{I_j}\\tag{2.3}\\] If we take \\(\\epsilon = - \\frac{1}{n-1}\\) we get a distribution with no weight on the point \\(x_j\\) and weight \\(\\frac{1}{n-1}\\) on the rest of the sample, \\(\\hat{F_{-j}}\\). In practice this is having a sample of size \\(n-1\\) by omitting \\(x_j\\) from the original sample. So, the jackknife aproximation to the empirical influence value \\(I_j\\) is: \\[I_{jack;j}=(n-1)[t(\\hat F)-t(\\hat F_{-j})]=(n-1)(\\theta-{\\hat\\theta_{-j}})\\] Consenquently: \\[b_{jack}=- \\frac{1}{n}\\sum_{j=1}^n{I_{jack;j}}\\tag{2.1}\\] \\[Var_{jack}=\\frac{1}{n(n-1)}(\\sum_{j=1}^n{I_{jack;j}^2-nb_{jack}^2)}\\tag{2.2}\\] sample&lt;-c(7.0,3.5,11.9,8.9,10.1,1.2,1.1,7.9,12.9,1.3,5.2,5.1,3.9,2.5,10.4,6.2,-3.9) n &lt;- length(sample) sample.mean &lt;- mean(sample) ljack &lt;- function(idx, sample, n){ (n-1)*(norm.maximLikelihoodEst.var(sample) - norm.maximLikelihoodEst.var(sample[-idx])) } bias.jackknife &lt;- -mean(sapply(1:n, ljack, sample, n)) bias.jackknife ## [1] -1.178088 ljack.bias.distribution &lt;- sapply(1:n, ljack, sample, n ) variance.jackknife &lt;- 1/(n*(n-1))*sum(sapply(1:n,function(idx,sample,n){ ljack(idx,sample,n)^2 - n*(bias.jackknife^2) },sample,n)) variance.jackknife ## [1] 35.37843 plot( density(-ljack.bias.distribution) , main=paste0(&quot;Density Estimation of Jackknife Bias (mean = &quot;,round(bias.jackknife,3),&quot; )&quot;) ) 6.3 Kolmogorov-Smirnov Test Consider the null hypothesis \\(H_0\\): The sample was generated from a random variable with a normal distribution with parameters \\((\\mu, \\sigma^2)\\) = \\((5,5)\\). Use the test statistic’s empirical distribution function to estimate the Kolmogorov-Smirnov test statistic’s p-value. Explain why the test statistic is adequate considering the type of null hypothesis we are trying to test. Hypothesis testing whose hypothesis are: \\[H_0:X\\sim \\mathcal{N}(5,5) \\\\ H_1:X\\not\\sim \\mathcal{N}(5,5)\\tag{1.0}\\] Since we dont know the probability distribution function of the test statistic, \\(T(X)=D\\), we will use the empirical distribution function under the null hypothesis, \\(H_0\\), \\(\\hat F(x)_{H_0}\\), \\[\\hat F(x)_{H_0}= \\frac{\\#\\{i:x_i \\leq x\\}}{n} \\tag{1.1}\\] with \\(x_i\\) being one of \\(n\\) observations of the sample To do this, we need to use the Monte Carlo method. So, we generate \\(m\\) samples of the random variable, \\(X\\) under \\(H_0\\), \\(X\\sim \\mathcal{N}(5,5)\\), and, for each sample, compute the test statistic: \\[x_1: x_1^1, ...,x_n^1 \\rightarrow t_1=T(x^1)\\\\.\\\\.\\\\.\\\\ x_m: x_1^m, ...,x_n^m \\rightarrow t_m=T(x^m)\\tag{1.2}\\] To compare the samples with the reference probability distribution, \\(\\mathcal{N}(5,5)\\), we will use the Kolmogorov-Smirnov statistic. In short, this statistic determines the degree of similarity between two given probability distributions through distance. This distance, D, is the the largest distance between the two given cumulative distribution functions. Being \\(F(x)\\) the cumulative distribution function of the known distribution and \\(y(j)\\) the discontinuity points of \\(\\hat F(x)_{H_0}\\): \\[D=\\max_x|\\hat F(x)_{H_0}-F(x)|\\] \\[=\\max_{x}\\{\\max_x\\{\\hat F(x)_{H_0}-F(x)\\},\\max_x\\{\\hat F(x)-F(x)_{H_0}\\} \\}\\] \\[=\\max_{j=1,...,n}\\{\\frac{j}{n}-F(y(j)),F(y(j))-\\frac{j-1}{n} \\}\\tag{1.3}\\] Figure 6.1: Graph showing the distances between continuous cdf and discrete empirical cdf (Ross, page 222, 4th edition) This is well ilustrated in figure 6.1, where since both functions are monotonically increasing, a maximum distance D will only occur in the discontinuity points of \\(\\hat F(x)_{H_0}\\) or \\(y(j)\\). As previously stated, D is applied to every Monte Carlo sample. So, appling \\((1.3)\\) to \\((1.1)\\) we get: \\[ T(x^1)=D_1=\\max_{j=1,...,n}\\{\\frac{j}{n}-F(y_1(j)),F(y_1(j))-\\frac{j-1}{n}\\} \\] \\[...\\] \\[ T(x^2)=D_2=\\max_{j=1,...,n}\\{\\frac{j}{n}-F(y_2(j)),F(y_2(j))-\\frac{j-1}{n}\\} \\tag{1.4} \\] and to the original sample: \\[T(x)=d=\\max_{j=1,...,n}\\{\\frac{j}{n}-F(y(j)),F(y(j))-\\frac{j-1}{n}\\}\\tag{1.5}\\] Finally, the \\(\\hat{p-value}\\) is estimated: \\[\\hat{p-value}=P(D\\geq d|H_0) \\\\ = \\frac{\\#\\{D_k:D_k \\geq d\\}}{n} \\tag{1.6}\\] KS_statDist&lt;-function(sample,mean,sd) { sample&lt;-sort(sample) n&lt;-length(sample) DVecA&lt;-rep(0, n) DVecB&lt;-rep(0, n) for(j in c(1:n)) { DVecA[j]&lt;-(j/n)-pnorm(sample[j],mean,sd) DVecB[j]&lt;-pnorm(sample[j],mean,sd)-((j-1)/n) } DVecAA &lt;- sapply(1:n,function(idx){(j/n)-pnorm(sample[j],mean,sd)}) DVecBB &lt;- sapply(1:n,function(idx){pnorm(sample[j],mean,sd)-((j-1)/n)}) return(max(c(DVecA,DVecB))) } #Kolmogorov-Smirnov test sample.sort &lt;- sort(sample,decreasing = F) d&lt;-KS_statDist(sample.sort,5,sqrt(5)) #plot EmpCDF&lt;-ecdf(sample.sort) NormCDF&lt;-pnorm(sample.sort,5,sqrt(5)) plot(EmpCDF,main=&quot;Kolmogorov-Smirnov Test&quot;) lines(sample.sort,NormCDF,col=&quot;red&quot;) MCsampleNum&lt;-2000 m&lt;-length(sample.sort) DVec&lt;-rep(0, m) pValueNumerator&lt;-0 for (k in c(1:MCsampleNum)) { #D calculation of the generated MC samples DVec[k]&lt;-KS_statDist(rnorm(m,5,sqrt(5)),5,sqrt(5)) if(DVec[k]&gt;=d) pValueNumerator&lt;-pValueNumerator+1 } DVec.df &lt;- data.frame( pts = DVec ) library(ggplot2) p &lt;- ggplot2::ggplot(DVec.df, aes(x=pts)) + geom_density() plot(density(DVec)) #p-value calculation pValue&lt;-(pValueNumerator+1)/(MCsampleNum+1) pValue ## [1] 0.181909 "],
["problem-2.html", "7 Problem 2 7.1 Exponential parameter MLE 7.2 Bootstrap bias and variance 7.3 Bootstrap confidence interval 7.4 Studentized-Bootstrap confidence interval 7.5 Discussion", " 7 Problem 2 7.1 Exponential parameter MLE Assume that the given sample originated from a random variable with a exponential distribution whose parameter \\(\\lambda\\) is unknown. Use the sample to determine the maximum-likelihood estimator of the parameter \\(\\lambda\\) Maximum normal distribution Likelihood estimation. Assuming that our sample follows a exponencial distribution as said on the exercise question, we can say that the probability density function follows this format \\[ f(x;\\lambda)=\\lambda e^{-\\lambda x} \\] And to optain the maximum likelihood estimation, we need to derivate the likelihood function in order to \\(\\lambda\\) and discover the zeros. \\[ L(\\lambda,x_1,...,x_n)=\\prod_{j=1}^n f_X(x_j;\\lambda) \\] \\[ =\\prod_{j=1}^n\\lambda e^{-\\lambda x_j} \\] \\[ =\\lambda e^{-\\lambda \\sum_{j=1}^n x_j} \\] \\[ l(\\lambda;x)= n\\ ln(\\lambda)-\\lambda(\\sum_{j=1}^n x_j) \\] \\[ \\frac{d}{d\\lambda}l(\\lambda;x)=0 \\equiv \\frac{n}{\\lambda}-\\sum_{j=1}^{n}x_j=0\\equiv\\lambda=\\frac{1}{\\frac{\\sum_{j=1}^n x_j}{n}} \\] which means \\[ \\hat{\\lambda}=\\frac{1}{\\overline{X}} \\] sample &lt;- c(0.05,0.03,0.19,0.14,0.12,0.03,0.08,0.19,0.07,0.01,0.24,0.10,0.03,0.31) exp.maximLikelihoodEst.lambda &lt;- function(sample){ 1/mean(sample) } exp.maximLikelihoodEst.lambda.estimated &lt;- exp.maximLikelihoodEst.lambda(sample) exp.maximLikelihoodEst.lambda.estimated ## [1] 8.805031 7.2 Bootstrap bias and variance Determine the Bootstrap bias and variance of the estimator obtained in the item (a). Viés: \\[E(\\hat{\\Psi}-\\Psi)\\approx b_{boot}(\\hat{\\Psi})= \\frac{1}{R}.\\sum_{r=1}^R.(\\hat{\\Psi}_r^*-\\hat{\\Psi})\\] R &lt;- 999 set.seed(1) bias.bootstrap &lt;- mean(sapply(1:R, function(idx,sample,sample.psi){ sample.bootstrap &lt;- rexp(length(sample),sample.psi) sample.bootstrap.psi &lt;- exp.maximLikelihoodEst.lambda(sample.bootstrap) sample.bootstrap.psi - sample.psi },sample,exp.maximLikelihoodEst.lambda.estimated)) bias.bootstrap ## [1] 0.7442572 \\[ Var(\\hat{\\Psi}-\\Psi)\\approx Var_{boot}=\\frac{1}{R-1}\\sum_{r=1}^R*(\\hat{\\Psi}_r^*-\\hat{\\Psi}^*)^2 \\] set.seed(1) bootstrap.var &lt;- function(sample,R){ estimated.bootstrap &lt;- sapply(1:R, function(idx,sample,sample.psi){ exp.maximLikelihoodEst.lambda(rexp(length(sample),sample.psi)) },sample,exp.maximLikelihoodEst.lambda.estimated) sum((estimated.bootstrap - mean(estimated.bootstrap))^2)/(R-1) } variance.bootstrap &lt;- bootstrap.var(sample,R) variance.bootstrap ## [1] 8.171176 7.3 Bootstrap confidence interval Construct a basic Bootstrap confidence interval for \\(\\lambda\\) Confidence interval \\[ P(a_{\\alpha} &lt; \\hat{\\theta}-\\theta &lt; a_{1-\\alpha}) \\] The confidence Interval can be describe like this: \\(]\\hat{\\theta}-a_{1-\\alpha},\\hat{\\theta}-a_{\\alpha}[\\) But,as we do not know the distribution of \\(\\hat{\\theta}-\\theta\\) we will use the empirical distribution function of \\(\\hat{\\theta^*}-\\hat{\\theta}\\) to estimate \\(a_{\\alpha}\\) and \\(a_{1-\\alpha}\\) That being said: \\(\\hat{a_\\alpha}=\\theta_{((R+1)\\alpha)}^*-\\hat{\\theta}\\) \\(\\hat{a_{\\alpha-1}}=\\theta_{((R+1)(\\alpha-1))}^*-\\hat{\\theta}\\) set.seed(1) boot.ci &lt;- function(sample, R, alphaG, exp.maximLikelihoodEst.lambda.estimated, variance.bootstrap){ alpha &lt;- alphaG/2 distance.booststrap &lt;- sapply(1:R, function(idx,sample,sample.phi){ sample.bootstrap &lt;- rexp(length(sample),sample.phi) sample.bootstrap.phi &lt;- exp.maximLikelihoodEst.lambda(sample.bootstrap) sample.bootstrap.phi - sample.phi },sample,exp.maximLikelihoodEst.lambda.estimated) distance.booststrap &lt;- c(distance.booststrap,0) distance.booststrap.sorted &lt;- sort(distance.booststrap, decreasing = F) ahat_left &lt;- distance.booststrap.sorted[(R+1)*alpha] ahat_right &lt;- distance.booststrap.sorted[(R+1)*(1-alpha)] sample.bootstraps.phi &lt;- distance.booststrap+exp.maximLikelihoodEst.lambda.estimated ci_left &lt;- exp.maximLikelihoodEst.lambda.estimated-ahat_right ci_right &lt;- exp.maximLikelihoodEst.lambda.estimated-ahat_left return( list( sample.bootstraps.phi = sample.bootstraps.phi, ci_left = ci_left, ci_right = ci_right ) ) } boot_cis &lt;- boot.ci(sample, R, 0.025, exp.maximLikelihoodEst.lambda.estimated, variance.bootstrap) sample.bootstraps.phi &lt;- boot_cis$sample.bootstraps.phi ## left value: 0.204453 ## right value: 12.595924 Figure 7.1: Confidence intervals of 0.025 in the estimate pdf of the estimator \\(\\lambda\\) 7.4 Studentized-Bootstrap confidence interval Construct a Studentized-Bootstrap confidence interval for \\(\\lambda\\) (Use the Bootstrap variance estimator to estimate the Bootstrap samples’s variance). Studentized-Bootstrap confidence Interval can be describe like this: \\[ ]\\ \\hat{\\theta}-z_{((R+1)(1-\\alpha))}^*\\sqrt v \\ ,\\ \\hat{\\theta}-z_{((R+1)\\alpha)}^r\\sqrt v \\ [ \\] The pivolal statistic used is the following: \\[ \\hat{z}=\\frac{\\hat{\\theta}-\\theta}{\\sqrt{var(\\hat{\\theta}-\\theta)}} \\] assuming that \\(var(\\hat{\\theta}-\\theta)\\) can be estimated by \\(Var_{boot}(\\hat{\\theta})\\) then \\[ \\hat{z_r^*}=\\frac{\\hat{\\theta_r^*}-\\hat{\\theta}}{\\sqrt{v_r^*}} \\] This method can be more accurate when compared to the basic bootstrap interval, while being more computational demanding[@cmu-bootstrap]. R &lt;- 999 set.seed(1) studentized.ci &lt;- function(sample, R,alphaG,exp.maximLikelihoodEst.lambda.estimated,variance.bootstrap){ alpha &lt;- alphaG/2 distance.booststrap.studantized &lt;- sapply(1:R, function(idx,sample,sample.phi){ sample.bootstrap &lt;- rexp(length(sample),sample.phi) sample.bootstrap.phi &lt;- exp.maximLikelihoodEst.lambda(sample.bootstrap) v &lt;- bootstrap.var(sample.bootstrap,R) (sample.bootstrap.phi - sample.phi)/sqrt(v) },sample,exp.maximLikelihoodEst.lambda.estimated) distance.booststrap.studantized &lt;- c(distance.booststrap.studantized,0) distance.booststrap.studantized.sorted &lt;- sort(distance.booststrap.studantized, decreasing = F) ahat.stdzd_left &lt;- distance.booststrap.studantized.sorted[(R+1)*alpha]*sqrt(variance.bootstrap) ahat.stdzd_right &lt;- distance.booststrap.studantized.sorted[(R+1)*(1-alpha)]*sqrt(variance.bootstrap) ci_left.stdzd &lt;- exp.maximLikelihoodEst.lambda.estimated-ahat.stdzd_right ci_right.stdzd &lt;- exp.maximLikelihoodEst.lambda.estimated-ahat.stdzd_left return( list( ci_left = ci_left.stdzd, ci_right = ci_right.stdzd ) ) } studentized.ci.values &lt;- studentized.ci(sample, R, 0.025, exp.maximLikelihoodEst.lambda.estimated, variance.bootstrap) ## left value: 0.190189 ## right value: 12.633102 Figure 7.2: Studentized confidence intervals of 0.025 in the estimate pdf of the estimator \\(\\lambda\\) 7.5 Discussion Discuss the results of items (c) and (d). Choose \\(\\alpha = 0.025\\) as the significance level for the confidence intervals. The values in these confidence intervals are pretty close, with the estimated value obtain in 6.1. From [@cmu-bootstrap], the studentized-bootstrap can be more accurate when comparing to the bootstrap, but in our simulation this was not evident: range of bootstrap: 12.391 range of studentized-bootstrap: 12.443 But by doing more simulations to assess this statement we can check this: set.seed(100) ranges &lt;- sapply(1:100,function(idx){ variance.bootstrap &lt;- bootstrap.var(sample,R) boot_cis &lt;- boot.ci(sample, R, 0.025, exp.maximLikelihoodEst.lambda.estimated, variance.bootstrap) std_cis &lt;- studentized.ci(sample, R, 0.025, exp.maximLikelihoodEst.lambda.estimated, variance.bootstrap) return( list( range.boot = boot_cis$ci_right - boot_cis$ci_left, range.boot.t = std_cis$ci_right - std_cis$ci_left ) ) }) ranges.unlist &lt;- unlist(ranges) boot.mean &lt;- mean(ranges.unlist[seq(1,length(ranges.unlist),2)]) boot.t.mean &lt;- mean(ranges.unlist[seq(2,length(ranges.unlist),2)]) boot.mean boot.t.mean 12.35 12.22 Given the results, we can see that, in average, the Studentized-Bootstrap can indeed be more accurate than the Boostrap, but by a small value. This small value in difference can be because of two things: Due to the small size of the sample (maybe they are not representative, which would inject a lot of uncertainty) Due to the characteristics of the data. The Studentized Boostrap is not always more accurate than the Boostrap, in spite of there are cases where it is. "]
]
