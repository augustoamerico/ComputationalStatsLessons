[
["problem-1.html", "Computational Stats Deliverable 2 1 Problem 1 1.1 Normal variance MLE 1.2 Jackknife bias and variance 1.3 Kolmogorov-Smirnov Test", " Computational Stats Deliverable 2 António Coelho, Gonçalo Arsénio, Sara Vigário, Tiago dos Santos 2018-11-09 1 Problem 1 Consider the following sample: c(7.0,3.5,11.9,8.9,10.1,1.2,1.1,7.9,12.9,1.3,5.2,5.1,3.9,2.5,10.4,6.2,-3.9) 1.1 Normal variance MLE Assume that the given sample originated from a random variable with a normal distribution whose parameter \\(\\sigma^2\\) is unknown. Use the sample to determine the maximum-likelihood estimator of the parameter \\(\\sigma^2\\). Maximum normal distribution Likelihood estimation. Assuming that our sample follows a normal distribution as said on the exercise question, we can say that the probability density function follows this format \\[ f_X(x_j)=(2. \\pi. \\sigma_0^2).e^{-\\frac{1}{2}.\\frac{(xj-\\mu)^2}{\\sigma_0^2}} \\] And to optain the maximum likelihood estimation, we need to partially derivate the likelihood in order to the two parameters, \\(\\mu\\) and \\(\\sigma^2\\), and discover the zeros. \\[ L(\\mu,\\sigma^2,x_1,...,x_n)=\\prod_{j=1}^n f_X(x_j;\\mu,\\sigma^2) \\] \\[ =\\prod_{j=1}^n(2\\pi\\sigma^2)^{-1/2}.e^{-\\frac{1}{2}.\\frac{(xj-\\mu)^2}{\\sigma^2}} \\] \\[ =(2\\pi\\sigma^2)^{-n/2}.e^{-\\frac{1}{2\\sigma^2}\\sum_{j=1}^{n}(xj-\\mu)^2} \\] Likelihood function: \\[ l(\\mu,\\sigma^2;x_1,...,x_n)=-\\frac{n}{2}ln(2\\pi)-\\frac{n}{2}ln(\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum_{j=1}^n(x_j-\\mu)^2 \\] Derivates equals to zero \\[ \\frac{\\partial}{\\partial \\mu}l(\\mu,\\sigma^2;x_1,...,x_n)=0 \\] \\[ \\frac{\\partial}{\\partial \\mu}l(\\mu,\\sigma^2;x_1,...,x_n)=\\frac{\\partial}{\\partial \\mu}\\bigg(-\\frac{n}{2}ln(2\\pi)-\\frac{n}{2}ln(\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum_{j=1}^n(x_j-\\mu)^2\\bigg)=\\frac{1}{\\sigma^2}(\\sum_{j=1}^nx_j-n\\mu) \\] \\[ \\frac{1}{\\sigma^2}(\\sum_{j=1}^nx_j-n\\mu)=0 \\equiv\\bigg(\\sum_{j=1}^nx_j-n\\mu\\bigg)=0 \\] Then: \\[ \\hat{\\mu}=\\frac{1}{n}\\bigg(\\sum_{j=1}^nx_j\\bigg) \\] \\[ \\frac{\\partial}{\\partial \\sigma^2}l(\\mu,\\sigma^2;x_1,...,x_n)=0 \\] \\[ \\frac{\\partial}{\\partial \\sigma^2}l(\\mu,\\sigma^2;x_1,...,x_n)=\\frac{\\partial}{\\partial \\mu}\\bigg(-\\frac{n}{2}ln(2\\pi)-\\frac{n}{2}ln(\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum_{j=1}^n(x_j-\\mu)^2)\\bigg)=\\frac{1}{2\\sigma^2}\\bigg[\\frac{1}{\\sigma^2}\\sum_{j=1}^{n}(x_j-\\mu)^2-n\\bigg] \\] \\[ \\frac{1}{2\\sigma^2}\\bigg[\\frac{1}{\\sigma^2}\\sum_{j=1}^{n}(x_j-\\mu)^2-n\\bigg]=0\\equiv\\frac{1}{\\sigma^2}\\sum_{j=1}^{n}(x_j-\\mu)^2-n=0 \\] Then: \\[ \\widehat{\\sigma^2}=\\frac{1}{n}\\sum_{j=1}^{n}(x_j-\\mu)^2 \\] sample&lt;-c(7.0,3.5,11.9,8.9,10.1,1.2,1.1,7.9,12.9,1.3,5.2,5.1,3.9,2.5,10.4,6.2,-3.9) norm.maximLikelihoodEst.var &lt;- function(sample){ n &lt;- length(sample) sample.mean &lt;- mean(sample) sum((sample - sample.mean)^2)/n } norm.maximLikelihoodEst.var(sample) ## [1] 18.84941 1.2 Jackknife bias and variance Determine the Jackknife bias and variance of the estimator obtained in the item (a), and discuss the quality of the estimator. We want to estimate the bias and variability of the estimator, \\(\\theta=t(F)\\). Since a good estimator not sensible to the sample is often of interest to know it faces minor pertubations in \\(F\\). We can define: \\[ L_t(y;F)=\\lim_{\\epsilon \\rightarrow 0} \\frac{t[(1-\\epsilon)F+\\epsilon H_y]-t(F)}{\\epsilon} \\] \\[ H_y= \\left\\{ \\begin{array}{ll} 1, u \\leq y \\\\ 0, u \\geq y \\end{array} \\right.\\tag{2.0} \\] as the influence fuction of \\(t\\) at \\(F\\). Using \\((2.0)\\) with the empirical function we get the empirical influence function: \\[ I(y)=L_t(y;\\hat F)\\tag{2.1} \\] Applying an extension of Taylor’s Theorem to \\(t(\\hat F)\\): \\[t(\\hat F) \\simeq t(F)+\\frac{1}{n}\\sum_{j=1}^n{I_j}\\tag{2.2}\\] So: \\[\\theta-\\hat{\\theta}=-\\frac{1}{n} \\sum_{j=1}^n{I_j}\\tag{2.3}\\] If we take \\(\\epsilon = - \\frac{1}{n-1}\\) we get a distribution with no weight on the point \\(x_j\\) and weight \\(\\frac{1}{n-1}\\) on the rest of the sample, \\(\\hat{F_{-j}}\\). In practice this is having a sample of size \\(n-1\\) by omitting \\(x_j\\) from the original sample. So, the jackknife aproximation to the empirical influence value \\(I_j\\) is: \\[I_{jack;j}=(n-1)[t(\\hat F)-t(\\hat F_{-j})]=(n-1)(\\theta-{\\hat\\theta_{-j}})\\] Consenquently: \\[b_{jack}=- \\frac{1}{n}\\sum_{j=1}^n{I_{jack;j}}\\tag{2.1}\\] \\[Var_{jack}=\\frac{1}{n(n-1)}(\\sum_{j=1}^n{I_{jack;j}^2-nb_{jack}^2)}\\tag{2.2}\\] sample&lt;-c(7.0,3.5,11.9,8.9,10.1,1.2,1.1,7.9,12.9,1.3,5.2,5.1,3.9,2.5,10.4,6.2,-3.9) n &lt;- length(sample) sample.mean &lt;- mean(sample) ljack &lt;- function(idx, sample, n){ (n-1)*(norm.maximLikelihoodEst.var(sample) - norm.maximLikelihoodEst.var(sample[-idx])) } bias.jackknife &lt;- -mean(sapply(1:n, ljack, sample, n)) bias.jackknife ## [1] -1.178088 ljack.bias.distribution &lt;- sapply(1:n, ljack, sample, n ) variance.jackknife &lt;- 1/(n*(n-1))*sum(sapply(1:n,function(idx,sample,n){ ljack(idx,sample,n)^2 - n*(bias.jackknife^2) },sample,n)) variance.jackknife ## [1] 35.37843 plot( density(-ljack.bias.distribution) , main=paste0(&quot;Density Estimation of Jackknife Bias (mean = &quot;,round(bias.jackknife,3),&quot; )&quot;) ) 1.3 Kolmogorov-Smirnov Test Consider the null hypothesis \\(H_0\\): The sample was generated from a random variable with a normal distribution with parameters \\((\\mu, \\sigma^2)\\) = \\((5,5)\\). Use the test statistic’s empirical distribution function to estimate the Kolmogorov-Smirnov test statistic’s p-value. Explain why the test statistic is adequate considering the type of null hypothesis we are trying to test. Hypothesis testing whose hypothesis are: \\[H_0:X\\sim \\mathcal{N}(5,5) \\\\ H_1:X\\not\\sim \\mathcal{N}(5,5)\\tag{1.0}\\] Since we dont know the probability distribution function of the test statistic, \\(T(X)=D\\), we will use the empirical distribution function under the null hypothesis, \\(H_0\\), \\(\\hat F(x)_{H_0}\\), \\[\\hat F(x)_{H_0}= \\frac{\\#\\{i:x_i \\leq x\\}}{n} \\tag{1.1}\\] with \\(x_i\\) being one of \\(n\\) observations of the sample To do this, we need to use the Monte Carlo method. So, we generate \\(m\\) samples of the random variable, \\(X\\) under \\(H_0\\), \\(X\\sim \\mathcal{N}(5,5)\\), and, for each sample, compute the test statistic: \\[x_1: x_1^1, ...,x_n^1 \\rightarrow t_1=T(x^1)\\\\.\\\\.\\\\.\\\\ x_m: x_1^m, ...,x_n^m \\rightarrow t_m=T(x^m)\\tag{1.2}\\] To compare the samples with the reference probability distribution, \\(\\mathcal{N}(5,5)\\), we will use the Kolmogorov-Smirnov statistic. In short, this statistic determines the degree of similarity between two given probability distributions through distance. This distance, D, is the the largest distance between the two given cumulative distribution functions. Being \\(F(x)\\) the cumulative distribution function of the known distribution and \\(y(j)\\) the discontinuity points of \\(\\hat F(x)_{H_0}\\): \\[D=\\max_x|\\hat F(x)_{H_0}-F(x)|\\] \\[=\\max_{x}\\{\\max_x\\{\\hat F(x)_{H_0}-F(x)\\},\\max_x\\{\\hat F(x)-F(x)_{H_0}\\} \\}\\] \\[=\\max_{j=1,...,n}\\{\\frac{j}{n}-F(y(j)),F(y(j))-\\frac{j-1}{n} \\}\\tag{1.3}\\] Figure 1.1: Graph showing the distances between continuous cdf and discrete empirical cdf (Ross, page 222, 4th edition) This is well ilustrated in figure 1.1, where since both functions are monotonically increasing, a maximum distance D will only occur in the discontinuity points of \\(\\hat F(x)_{H_0}\\) or \\(y(j)\\). As previously stated, D is applied to every Monte Carlo sample. So, appling \\((1.3)\\) to \\((1.1)\\) we get: \\[ T(x^1)=D_1=\\max_{j=1,...,n}\\{\\frac{j}{n}-F(y_1(j)),F(y_1(j))-\\frac{j-1}{n}\\} \\] \\[...\\] \\[ T(x^2)=D_2=\\max_{j=1,...,n}\\{\\frac{j}{n}-F(y_2(j)),F(y_2(j))-\\frac{j-1}{n}\\} \\tag{1.4} \\] and to the original sample: \\[T(x)=d=\\max_{j=1,...,n}\\{\\frac{j}{n}-F(y(j)),F(y(j))-\\frac{j-1}{n}\\}\\tag{1.5}\\] Finally, the \\(\\hat{p-value}\\) is estimated: \\[\\hat{p-value}=P(D\\geq d|H_0) \\\\ = \\frac{\\#\\{D_k:D_k \\geq d\\}}{n} \\tag{1.6}\\] KS_statDist&lt;-function(sample,mean,sd) { sample&lt;-sort(sample) n&lt;-length(sample) DVecA&lt;-rep(0, n) DVecB&lt;-rep(0, n) for(j in c(1:n)) { DVecA[j]&lt;-(j/n)-pnorm(sample[j],mean,sd) DVecB[j]&lt;-pnorm(sample[j],mean,sd)-((j-1)/n) } DVecAA &lt;- sapply(1:n,function(idx){(j/n)-pnorm(sample[j],mean,sd)}) DVecBB &lt;- sapply(1:n,function(idx){pnorm(sample[j],mean,sd)-((j-1)/n)}) return(max(c(DVecA,DVecB))) } #Kolmogorov-Smirnov test sample.sort &lt;- sort(sample,decreasing = F) d&lt;-KS_statDist(sample.sort,5,sqrt(5)) #plot EmpCDF&lt;-ecdf(sample.sort) NormCDF&lt;-pnorm(sample.sort,5,sqrt(5)) plot(EmpCDF,main=&quot;Kolmogorov-Smirnov Test&quot;) lines(sample.sort,NormCDF,col=&quot;red&quot;) MCsampleNum&lt;-2000 m&lt;-length(sample.sort) DVec&lt;-rep(0, m) pValueNumerator&lt;-0 for (k in c(1:MCsampleNum)) { #D calculation of the generated MC samples DVec[k]&lt;-KS_statDist(rnorm(m,5,sqrt(5)),5,sqrt(5)) if(DVec[k]&gt;=d) pValueNumerator&lt;-pValueNumerator+1 } DVec.df &lt;- data.frame( pts = DVec ) library(ggplot2) p &lt;- ggplot2::ggplot(DVec.df, aes(x=pts)) + geom_density() plot(density(DVec)) #p-value calculation pValue&lt;-(pValueNumerator+1)/(MCsampleNum+1) pValue ## [1] 0.2118941 "],
["problem-2.html", "2 Problem 2 2.1 Exponential parameter MLE 2.2 Bootstrap bias and variance 2.3 Bootstrap confidence interval 2.4 Studentized-Bootstrap confidence interval 2.5 Discussion", " 2 Problem 2 2.1 Exponential parameter MLE Assume that the given sample originated from a random variable with a exponential distribution whose parameter \\(\\lambda\\) is unknown. Use the sample to determine the maximum-likelihood estimator of the parameter \\(\\lambda\\) Maximum normal distribution Likelihood estimation. Assuming that our sample follows a exponencial distribution as said on the exercise question, we can say that the probability density function follows this format \\[ f(x;\\lambda)=\\lambda e^{-\\lambda x} \\] And to optain the maximum likelihood estimation, we need to derivate the likelihood function in order to \\(\\lambda\\) and discover the zeros. \\[ L(\\lambda,x_1,...,x_n)=\\prod_{j=1}^n f_X(x_j;\\lambda) \\] \\[ =\\prod_{j=1}^n\\lambda e^{-\\lambda x_j} \\] \\[ =\\lambda e^{-\\lambda \\sum_{j=1}^n x_j} \\] \\[ l(\\lambda;x)= n\\ ln(\\lambda)-\\lambda(\\sum_{j=1}^n x_j) \\] \\[ \\frac{d}{d\\lambda}l(\\lambda;x)=0 \\equiv \\frac{n}{\\lambda}-\\sum_{j=1}^{n}x_j=0\\equiv\\lambda=\\frac{1}{\\frac{\\sum_{j=1}^n x_j}{n}} \\] which means \\[ \\hat{\\lambda}=\\frac{1}{\\overline{X}} \\] sample &lt;- c(0.05,0.03,0.19,0.14,0.12,0.03,0.08,0.19,0.07,0.01,0.24,0.10,0.03,0.31) exp.maximLikelihoodEst.lambda &lt;- function(sample){ 1/mean(sample) } exp.maximLikelihoodEst.lambda.estimated &lt;- exp.maximLikelihoodEst.lambda(sample) exp.maximLikelihoodEst.lambda.estimated ## [1] 8.805031 2.2 Bootstrap bias and variance Determine the Bootstrap bias and variance of the estimator obtained in the item (a). Viés: \\[E(\\hat{\\Psi}-\\Psi)\\approx b_{boot}(\\hat{\\Psi})= \\frac{1}{R}.\\sum_{r=1}^R.(\\hat{\\Psi}_r^*-\\hat{\\Psi})\\] R &lt;- 999 set.seed(1) bias.bootstrap &lt;- mean(sapply(1:R, function(idx,sample,sample.psi){ sample.bootstrap &lt;- rexp(length(sample),sample.psi) sample.bootstrap.psi &lt;- exp.maximLikelihoodEst.lambda(sample.bootstrap) sample.bootstrap.psi - sample.psi },sample,exp.maximLikelihoodEst.lambda.estimated)) bias.bootstrap ## [1] 0.7442572 \\[ Var(\\hat{\\Psi}-\\Psi)\\approx Var_{boot}=\\frac{1}{R-1}\\sum_{r=1}^R*(\\hat{\\Psi}_r^*-\\hat{\\Psi}^*)^2 \\] set.seed(1) bootstrap.var &lt;- function(sample,R){ estimated.bootstrap &lt;- sapply(1:R, function(idx,sample,sample.psi){ exp.maximLikelihoodEst.lambda(rexp(length(sample),sample.psi)) },sample,exp.maximLikelihoodEst.lambda.estimated) sum((estimated.bootstrap - mean(estimated.bootstrap))^2)/(R-1) } variance.bootstrap &lt;- bootstrap.var(sample,R) variance.bootstrap ## [1] 8.171176 2.3 Bootstrap confidence interval Construct a basic Bootstrap confidence interval for \\(\\lambda\\) Confidence interval \\[ P(a_{\\alpha} &lt; \\hat{\\theta}-\\theta &lt; a_{1-\\alpha}) \\] The confidence Interval can be describe like this: \\(]\\hat{\\theta}-a_{1-\\alpha},\\hat{\\theta}-a_{\\alpha}[\\) But,as we do not know the distribution of \\(\\hat{\\theta}-\\theta\\) we will use the empirical distribution function of \\(\\hat{\\theta^*}-\\hat{\\theta}\\) to estimate \\(a_{\\alpha}\\) and \\(a_{1-\\alpha}\\) That being said: \\(\\hat{a_\\alpha}=\\theta_{((R+1)\\alpha)}^*-\\hat{\\theta}\\) \\(\\hat{a_{\\alpha-1}}=\\theta_{((R+1)(\\alpha-1))}^*-\\hat{\\theta}\\) set.seed(1) boot.ci &lt;- function(sample, R, alphaG, exp.maximLikelihoodEst.lambda.estimated, variance.bootstrap){ alpha &lt;- alphaG/2 distance.booststrap &lt;- sapply(1:R, function(idx,sample,sample.phi){ sample.bootstrap &lt;- rexp(length(sample),sample.phi) sample.bootstrap.phi &lt;- exp.maximLikelihoodEst.lambda(sample.bootstrap) sample.bootstrap.phi - sample.phi },sample,exp.maximLikelihoodEst.lambda.estimated) distance.booststrap &lt;- c(distance.booststrap,0) distance.booststrap.sorted &lt;- sort(distance.booststrap, decreasing = F) ahat_left &lt;- distance.booststrap.sorted[(R+1)*alpha] ahat_right &lt;- distance.booststrap.sorted[(R+1)*(1-alpha)] sample.bootstraps.phi &lt;- distance.booststrap+exp.maximLikelihoodEst.lambda.estimated ci_left &lt;- exp.maximLikelihoodEst.lambda.estimated-ahat_right ci_right &lt;- exp.maximLikelihoodEst.lambda.estimated-ahat_left return( list( sample.bootstraps.phi = sample.bootstraps.phi, ci_left = ci_left, ci_right = ci_right ) ) } boot_cis &lt;- boot.ci(sample, R, 0.025, exp.maximLikelihoodEst.lambda.estimated, variance.bootstrap) sample.bootstraps.phi &lt;- boot_cis$sample.bootstraps.phi ## left value: 0.204453 ## right value: 12.595924 Figure 2.1: Density estimation of the pivot statistic \\(\\hat{\\theta - \\theta}\\) 2.4 Studentized-Bootstrap confidence interval Construct a Studentized-Bootstrap confidence interval for \\(\\lambda\\) (Use the Bootstrap variance estimator to estimate the Bootstrap samples’s variance). Studentized-Bootstrap confidence Interval can be describe like this: \\[ ]\\ \\hat{\\theta}-z_{((R+1)(1-\\alpha))}^*\\sqrt v \\ ,\\ \\hat{\\theta}-z_{((R+1)\\alpha)}^r\\sqrt v \\ [ \\] The pivolal statistic used is the following: \\[ \\hat{z}=\\frac{\\hat{\\theta}-\\theta}{\\sqrt{var(\\hat{\\theta}-\\theta)}} \\] assuming that \\(var(\\hat{\\theta}-\\theta)\\) can be estimated by \\(Var_{boot}(\\hat{\\theta})\\) then \\[ \\hat{z_r^*}=\\frac{\\hat{\\theta_r^*}-\\hat{\\theta}}{\\sqrt{v_r^*}} \\] This method can be more accurate when compared to the basic bootstrap interval, while being more computational demanding(Ryan Tibshirani 2014). R &lt;- 999 set.seed(1) studentized.ci &lt;- function(sample, R,alphaG,exp.maximLikelihoodEst.lambda.estimated,variance.bootstrap){ alpha &lt;- alphaG/2 distance.booststrap.studantized &lt;- sapply(1:R, function(idx,sample){ sample.phi &lt;- exp.maximLikelihoodEst.lambda(sample) sample.bootstrap &lt;- rexp(length(sample),sample.phi) sample.bootstrap.phi &lt;- exp.maximLikelihoodEst.lambda(sample.bootstrap) v &lt;- bootstrap.var(sample.bootstrap,R) (sample.bootstrap.phi - sample.phi)/sqrt(v) },sample) distance.booststrap.studantized &lt;- c(distance.booststrap.studantized,0) distance.booststrap.studantized.sorted &lt;- sort(distance.booststrap.studantized, decreasing = F) ahat.stdzd_left &lt;- distance.booststrap.studantized.sorted[(R+1)*alpha]*sqrt(variance.bootstrap) ahat.stdzd_right &lt;- distance.booststrap.studantized.sorted[(R+1)*(1-alpha)]*sqrt(variance.bootstrap) ci_left.stdzd &lt;- exp.maximLikelihoodEst.lambda.estimated-ahat.stdzd_right ci_right.stdzd &lt;- exp.maximLikelihoodEst.lambda.estimated-ahat.stdzd_left return( list( ci_left = ci_left.stdzd, ci_right = ci_right.stdzd ) ) } studentized.ci.values &lt;- studentized.ci(sample, R, 0.025, exp.maximLikelihoodEst.lambda.estimated, variance.bootstrap) ## left value: 0.190189 ## right value: 12.633102 Figure 2.2: Density estimation of the pivot statistic \\(z\\) 2.5 Discussion Discuss the results of items (c) and (d). Choose \\(\\alpha = 0.025\\) as the significance level for the confidence intervals. The values in these confidence intervals are pretty close, with the estimated value obtain in 1.1. From (Ryan Tibshirani 2014), the studentized-bootstrap can be more accurate when comparing to the bootstrap, but in our simulation this was not evident: range of bootstrap: 12.391 range of studentized-bootstrap: 12.443 But by doing more simulations to assess this statement we can check this: set.seed(100) ranges &lt;- sapply(1:100,function(idx){ variance.bootstrap &lt;- bootstrap.var(sample,R) boot_cis &lt;- boot.ci(sample, R, 0.025, exp.maximLikelihoodEst.lambda.estimated, variance.bootstrap) std_cis &lt;- studentized.ci(sample, R, 0.025, exp.maximLikelihoodEst.lambda.estimated, variance.bootstrap) return( list( range.boot = boot_cis$ci_right - boot_cis$ci_left, range.boot.t = std_cis$ci_right - std_cis$ci_left ) ) }) ranges.unlist &lt;- unlist(ranges) boot.mean &lt;- mean(ranges.unlist[seq(1,length(ranges.unlist),2)]) boot.t.mean &lt;- mean(ranges.unlist[seq(2,length(ranges.unlist),2)]) boot.mean boot.t.mean 12.35 12.22 Given the results, we can see that, in average, the Studentized-Bootstrap can indeed be more accurate than the Boostrap, but by a small value. But in this case, we have to take in consideration that the small size of the sample can influence this results - maybe with a larger sample we could assess this sentence with other confidence. "]
]
