---
title: "Computational Stats Deliverable 2"
author: "Group III: António Coelho, Gonçalo Arsénio, Sara Vigário, Tiago dos Santos"
date: "2018-11-09"
site: bookdown::bookdown_site
#header-includes:
#    - \usepackage{mathtools}
#    - \usepackage{amsmath}
output: pdf_document
---

```{r include = F, warning=FALSE}
library(tinytex)
library(ggplot2)
library(plotly)
#https://github.com/rstudio/rmarkdown/issues/1285

LatexOrOther <- function(latex, other){
  if (identical(knitr:::pandoc_to(), 'latex')) 
    latex 
  else other
}

PROJHOME <- "/Users/tiago.correia/courses/MestradoBigData/EstatisticaNumericaComputacional"
this.path = file.path(PROJHOME,"Works","work2")

if(!exists("GLOBAL.Section")){
  GLOBAL.Section <- ""
}

confidence_interval_plot <- function(sample, ci.left, ci.right){
  sample.df <- data.frame(
    sample = sample
  )
  ggplotdata <- ggplot2::ggplot_build(ggplot2::ggplot(sample.df, aes(x=sample)) + geom_density())
  plt.data <- ggplotdata$data[[1]]
  summary(plt.data)
  plt.data$density <- plt.data$density/sum(plt.data$density)
  plt.data$cumDens <- cumsum(plt.data$density)
  ahat_left.apprx <- approx(x=plt.data$x, y=plt.data$y,xout=ci.left)
  ahat_right.apprx <- approx(x=plt.data$x, y=plt.data$y,xout=ci.right)
  
  plt.area <- plt.data[plt.data$x >= ahat_left.apprx$x & plt.data$x <= ahat_right.apprx$x,1:5]
  #plt.area$density <- plt.area$density/sum(plt.data$density)
  
  plt.area.first <- plt.area[1,]
  plt.area.first$y=0
  plt.area.first$density=0
  plt.area.last <- plt.area[nrow(plt.area),]
  plt.area.last$y=0
  plt.area.last$density=0
  plt.area.edges <- rbind(plt.area.first,plt.area,plt.area.last)
  
  ci.df <- data.frame(
  x = c(ahat_left.apprx$x,ahat_right.apprx$x), y = c(0,0), xend = c(ahat_left.apprx$x,ahat_right.apprx$x), yend = c(ahat_left.apprx$y,ahat_right.apprx$y)
)
  
p <- ggplot2::ggplot(plt.data, aes(x=x,y=y)) +
      geom_polygon(data = plt.area.edges, aes(x = x, y = y, fill="CI"), alpha = .5) +
      geom_segment(aes(x = x, y = y, xend = xend, yend = yend, linetype="dotted", colour = "CI"),data=ci.df)+
      geom_line(aes(label=round(cumDens,3)))  
  
pp <- ggplotly(
    p
  ,tooltip = c("round(cumDens, 3)","x","y"))

return(LatexOrOther(p,pp))
}
```

`r GLOBAL.Section`# Problem 1

Consider the following sample:
```{r eval=F}
c(7.0,3.5,11.9,8.9,10.1,1.2,1.1,7.9,12.9,1.3,5.2,5.1,3.9,2.5,10.4,6.2,-3.9)
```

`r GLOBAL.Section`## Normal variance MLE {#sub:normal-mle-var}
Assume that the given sample originated from a random variable with a normal distribution whose parameter $\sigma^2$ is unknown. Use the sample to determine the maximum-likelihood estimator of the parameter $\sigma^2$.

_____

Maximum normal distribution Likelihood estimation.

Assuming that our sample follows a normal distribution as said on the exercise question, we can say that the probability density function is as following:
$$
f_X(x_j)=(2. \pi. \sigma_0^2).e^{-\frac{1}{2}.\frac{(xj-\mu)^2}{\sigma_0^2}}
$$

Now, we need to obtain the point with max value of this function, which will be our estimated value. And to obtain the maximum likelihood estimation, we need to partially derivate the likelihood function in order to the two parameters of the normal distribution, $\mu$ and $\sigma^2$, and:

1. Obtain the zeros
2. Check if those zeros are indeed a maximum

$$
L(\mu,\sigma^2,x_1,...,x_n)=\prod_{j=1}^n f_X(x_j;\mu,\sigma^2)
$$

$$
=\prod_{j=1}^n(2\pi\sigma^2)^{-1/2}.e^{-\frac{1}{2}.\frac{(xj-\mu)^2}{\sigma^2}}
$$

$$
=(2\pi\sigma^2)^{-n/2}.e^{-\frac{1}{2\sigma^2}\sum_{j=1}^{n}(xj-\mu)^2}
$$
Likelihood function:
$$
l(\mu,\sigma^2;x_1,...,x_n)=-\frac{n}{2}ln(2\pi)-\frac{n}{2}ln(\sigma^2)-\frac{1}{2\sigma^2}\sum_{j=1}^n(x_j-\mu)^2
$$
Derivates equals to zero
$$
\frac{\partial}{\partial \mu}l(\mu,\sigma^2;x_1,...,x_n)=0
$$

$$
\frac{\partial}{\partial \mu}l(\mu,\sigma^2;x_1,...,x_n)=\frac{\partial}{\partial \mu}\bigg(-\frac{n}{2}ln(2\pi)-\frac{n}{2}ln(\sigma^2)-\frac{1}{2\sigma^2}\sum_{j=1}^n(x_j-\mu)^2\bigg)=\frac{1}{\sigma^2}(\sum_{j=1}^nx_j-n\mu)
$$

$$
\frac{1}{\sigma^2}(\sum_{j=1}^nx_j-n\mu)=0 \equiv\bigg(\sum_{j=1}^nx_j-n\mu\bigg)=0
$$
Then:
$$
\hat{\mu}=\frac{1}{n}\bigg(\sum_{j=1}^nx_j\bigg)
$$

$$
\frac{\partial}{\partial \sigma^2}l(\mu,\sigma^2;x_1,...,x_n)=0
$$
$$
\frac{\partial}{\partial \sigma^2}l(\mu,\sigma^2;x_1,...,x_n)=\frac{\partial}{\partial \mu}\bigg(-\frac{n}{2}ln(2\pi)-\frac{n}{2}ln(\sigma^2)-\frac{1}{2\sigma^2}\sum_{j=1}^n(x_j-\mu)^2)\bigg)=\frac{1}{2\sigma^2}\bigg[\frac{1}{\sigma^2}\sum_{j=1}^{n}(x_j-\mu)^2-n\bigg]
$$
$$
\frac{1}{2\sigma^2}\bigg[\frac{1}{\sigma^2}\sum_{j=1}^{n}(x_j-\mu)^2-n\bigg]=0\equiv\frac{1}{\sigma^2}\sum_{j=1}^{n}(x_j-\mu)^2-n=0
$$
Then:
$$
\widehat{\sigma^2}=\frac{1}{n}\sum_{j=1}^{n}(x_j-\mu)^2
$$

```{r}
sample<-c(7.0,3.5,11.9,8.9,10.1,1.2,1.1,7.9,12.9,1.3,5.2,5.1,3.9,2.5,10.4,6.2,-3.9)

norm.maximLikelihoodEst.var <- function(sample){
  n <- length(sample)
  sample.mean <- mean(sample)
  sum((sample - sample.mean)^2)/n
}

norm.maximLikelihoodEst.var(sample)

```

`r GLOBAL.Section`## Jackknife bias and variance

Determine the Jackknife bias and variance of the estimator obtained in the item (a), and discuss the quality of the estimator.

____

We want to estimate the bias and variability of the estimator, $\theta=t(F)$. Since a good estimator not sensible to the sample is often of interest to know it faces minor pertubations in $F$. We can define:

$$
L_t(y;F)=\lim_{\epsilon \rightarrow 0} \frac{t[(1-\epsilon)F+\epsilon H_y]-t(F)}{\epsilon}
$$
$$
  H_y= \left\{
  \begin{array}{ll}
  1, u \leq y
  \\
  0, u \geq y
  \end{array}
  \right.\tag{2.0}
$$
  
as the influence fuction of $t$ at $F$.

Using $(2.0)$ with the empirical function we get the empirical influence function:

$$
  I(y)=L_t(y;\hat F)\tag{2.1}
$$
Applying an extension of Taylor's Theorem to $t(\hat F)$:

$$t(\hat F) \simeq t(F)+\frac{1}{n}\sum_{j=1}^n{I_j}\tag{2.2}$$
So:

$$\theta-\hat{\theta}=-\frac{1}{n} \sum_{j=1}^n{I_j}\tag{2.3}$$

If we take $\epsilon = - \frac{1}{n-1}$ we get a distribution with no weight on the point $x_j$ and weight $\frac{1}{n-1}$ on the rest of the sample, $\hat{F_{-j}}$. In practice this is having a sample of size $n-1$ by omitting $x_j$ from the original sample. 

So, the jackknife aproximation to the empirical influence value $I_j$ is:

$$I_{jack;j}=(n-1)[t(\hat F)-t(\hat F_{-j})]=(n-1)(\theta-{\hat\theta_{-j}})$$

Consenquently:

$$b_{jack}=- \frac{1}{n}\sum_{j=1}^n{I_{jack;j}}\tag{2.1}$$
$$Var_{jack}=\frac{1}{n(n-1)}(\sum_{j=1}^n{I_{jack;j}^2-nb_{jack}^2)}\tag{2.2}$$

```{r}
sample<-c(7.0,3.5,11.9,8.9,10.1,1.2,1.1,7.9,12.9,1.3,5.2,5.1,3.9,2.5,10.4,6.2,-3.9)
n <- length(sample)
sample.mean <- mean(sample)


ljack <- function(idx, sample, n){
  (n-1)*(norm.maximLikelihoodEst.var(sample) - norm.maximLikelihoodEst.var(sample[-idx]))
}

bias.jackknife <- -mean(sapply(1:n, ljack, sample, n))
bias.jackknife

ljack.bias.distribution <- sapply(1:n, ljack, sample, n )

variance.jackknife <- 1/(n*(n-1))*sum(sapply(1:n,function(idx,sample,n){
  ljack(idx,sample,n)^2 - n*(bias.jackknife^2)
},sample,n))
variance.jackknife

plot(
  density(-ljack.bias.distribution)
  , main=paste0("Density Estimation of Jackknife Bias (mean = ",round(bias.jackknife,3)," )")
)
```


```{r include = F}
## Acording Bradley Efron in The Jackknife, the bootstrap and other resampling plans
ljack.estimated.distribution <- sapply(1:n,function(idx,sample){
  norm.maximLikelihoodEst.var(sample[-idx])
},sample)

estimated.jackknife2 <- mean(sapply(1:n,function(idx,sample){
  norm.maximLikelihoodEst.var(sample[-idx])
},sample))

variance.jackknife2 <- ((n-1)/n)*sum(sapply(1:n,function(idx,sample){
   (norm.maximLikelihoodEst.var(sample[-idx]) - estimated.jackknife2)^2
},sample))
##
```


`r GLOBAL.Section`## Kolmogorov-Smirnov Test

Consider the null hypothesis $H_0$: The sample was generated from a random variable with a normal distribution with parameters   $(\mu, \sigma^2)$ = $(5,5)$. Use the test statistic's empirical distribution function  to estimate the Kolmogorov-Smirnov test statistic's p-value. Explain why the test statistic is adequate considering the type of null hypothesis we are trying to test.

____

Hypothesis testing whose hypothesis are:

 $$H_0:X\sim \mathcal{N}(5,5) \\ H_1:X\not\sim \mathcal{N}(5,5)\tag{1.0}$$

Since we dont know the probability distribution function of the test statistic, $T(X)=D$, we will use the empirical distribution function under the null hypothesis, $H_0$, $\hat F(x)_{H_0}$,


  $$\hat F(x)_{H_0}= \frac{\#\{i:x_i \leq x\}}{n} \tag{1.1}$$
  
  
with $x_i$ being one of $n$ observations of the sample

To do this, we need to use the Monte Carlo method. 

So, we generate $m$ samples of the random variable, $X$ under $H_0$, $X\sim \mathcal{N}(5,5)$, and, for each sample, compute the test statistic:

$$x_1: x_1^1, ...,x_n^1 \rightarrow t_1=T(x^1)\\.\\.\\.\\ x_m: x_1^m, ...,x_n^m \rightarrow t_m=T(x^m)\tag{1.2}$$

To compare the samples with the reference probability distribution, $\mathcal{N}(5,5)$, we will use the Kolmogorov-Smirnov statistic. 

In short, this statistic determines the degree of similarity between two given probability distributions through distance. This distance, D, is the the largest distance between the two given cumulative distribution functions. 

Being $F(x)$ the cumulative distribution function of the known distribution and $y(j)$ the discontinuity points of $\hat F(x)_{H_0}$:

$$D=\max_x|\hat F(x)_{H_0}-F(x)|$$
$$=\max_{x}\{\max_x\{\hat F(x)_{H_0}-F(x)\},\max_x\{\hat F(x)-F(x)_{H_0}\} \}$$
$$=\max_{j=1,...,n}\{\frac{j}{n}-F(y(j)),F(y(j))-\frac{j-1}{n} \}\tag{1.3}$$

```{r ross-ks, echo=F,fig.cap="Graph showing the distances between continuous cdf and discrete empirical cdf (Ross, page 222, 4th edition)",fig.env='figure'}
knitr::include_graphics(file.path("assets","ross_ks_test.png"))
```

This is well ilustrated in figure \@ref(fig:ross-ks), where since both functions are monotonically increasing, a maximum distance D will only occur in the discontinuity points of $\hat F(x)_{H_0}$ or $y(j)$.

As previously stated, D is applied to every Monte Carlo sample. So, appling $(1.3)$ to $(1.1)$ we get:

$$
T(x^1)=D_1=\max_{j=1,...,n}\{\frac{j}{n}-F(y_1(j)),F(y_1(j))-\frac{j-1}{n}\}
$$ 
$$...$$
$$
  T(x^2)=D_2=\max_{j=1,...,n}\{\frac{j}{n}-F(y_2(j)),F(y_2(j))-\frac{j-1}{n}\} \tag{1.4}
$$
and to the original sample:

$$T(x)=d=\max_{j=1,...,n}\{\frac{j}{n}-F(y(j)),F(y(j))-\frac{j-1}{n}\}\tag{1.5}$$

Finally, the $\hat{p-value}$ is estimated:

$$\hat{p-value}=P(D\geq d|H_0) \\ = \frac{\#\{D_k:D_k \geq d\}}{n} \tag{1.6}$$

```{r}
KS_statDist<-function(sample,mean,sd)
{
  sample<-sort(sample)
  n<-length(sample)
  DVecA<-rep(0, n)
  DVecB<-rep(0, n)
  for(j in c(1:n))
  {
    DVecA[j]<-(j/n)-pnorm(sample[j],mean,sd)
    DVecB[j]<-pnorm(sample[j],mean,sd)-((j-1)/n)
  }
  DVecAA <- sapply(1:n,function(idx){(j/n)-pnorm(sample[j],mean,sd)})
  DVecBB <- sapply(1:n,function(idx){pnorm(sample[j],mean,sd)-((j-1)/n)})
  return(max(c(DVecA,DVecB)))
}

#Kolmogorov-Smirnov test
sample.sort <- sort(sample,decreasing = F)
d<-KS_statDist(sample.sort,5,sqrt(5))

#plot
EmpCDF<-ecdf(sample.sort)
NormCDF<-pnorm(sample.sort,5,sqrt(5))
plot(EmpCDF,main="Kolmogorov-Smirnov Test")
lines(sample.sort,NormCDF,col="red")

MCsampleNum<-2000
m<-length(sample.sort)
DVec<-rep(0, m)
pValueNumerator<-0
for (k in c(1:MCsampleNum)) 
{
  #D calculation of the generated MC samples 
  DVec[k]<-KS_statDist(rnorm(m,5,sqrt(5)),5,sqrt(5))
  if(DVec[k]>=d)
    pValueNumerator<-pValueNumerator+1
}

DVec.df <- data.frame(
  pts = DVec
)
library(ggplot2)
p <- ggplot2::ggplot(DVec.df, aes(x=pts)) + 
  geom_density()
plot(density(DVec))

#p-value calculation
pValue<-(pValueNumerator+1)/(MCsampleNum+1)
pValue
```


`r GLOBAL.Section`# Problem 2

`r GLOBAL.Section`## Exponential parameter MLE

Assume that the given sample originated from a random variable with a exponential distribution whose parameter $\lambda$ is unknown. Use the sample to determine the maximum-likelihood estimator of the parameter $\lambda$ 

_____

Maximum normal distribution Likelihood estimation.

Assuming that our sample follows a exponencial distribution as said on the exercise question, we can say that the probability density function follows this format
$$
f(x;\lambda)=\lambda e^{-\lambda x}
$$
And to optain the maximum likelihood estimation, we need to derivate the likelihood function in order to $\lambda$ and discover the zeros.
$$
L(\lambda,x_1,...,x_n)=\prod_{j=1}^n f_X(x_j;\lambda)
$$
$$
=\prod_{j=1}^n\lambda e^{-\lambda x_j}
$$
$$
=\lambda e^{-\lambda \sum_{j=1}^n x_j}
$$
$$
l(\lambda;x)= n\ ln(\lambda)-\lambda(\sum_{j=1}^n x_j)
$$
$$
\frac{d}{d\lambda}l(\lambda;x)=0 \equiv \frac{n}{\lambda}-\sum_{j=1}^{n}x_j=0\equiv\lambda=\frac{1}{\frac{\sum_{j=1}^n x_j}{n}}
$$
which means
$$
\hat{\lambda}=\frac{1}{\overline{X}}
$$


```{r}
sample <- c(0.05,0.03,0.19,0.14,0.12,0.03,0.08,0.19,0.07,0.01,0.24,0.10,0.03,0.31)

exp.maximLikelihoodEst.lambda <- function(sample){
  1/mean(sample)
}

exp.maximLikelihoodEst.lambda.estimated <- exp.maximLikelihoodEst.lambda(sample)
exp.maximLikelihoodEst.lambda.estimated

```

`r GLOBAL.Section`## Bootstrap bias and variance

Determine the Bootstrap bias and variance of the estimator obtained in the item (a). 

______

Viés:
$$E(\hat{\Psi}-\Psi)\approx b_{boot}(\hat{\Psi})= \frac{1}{R}.\sum_{r=1}^R.(\hat{\Psi}_r^*-\hat{\Psi})$$


```{r bias-bootstrap}
R <- 999
set.seed(1)
bias.bootstrap <- mean(sapply(1:R, function(idx,sample,sample.psi){
  sample.bootstrap <- rexp(length(sample),sample.psi)
  sample.bootstrap.psi <- exp.maximLikelihoodEst.lambda(sample.bootstrap)
  
  sample.bootstrap.psi - sample.psi
},sample,exp.maximLikelihoodEst.lambda.estimated))
bias.bootstrap
```

$$
Var(\hat{\Psi}-\Psi)\approx Var_{boot}=\frac{1}{R-1}\sum_{r=1}^R*(\hat{\Psi}_r^*-\hat{\Psi}^*)^2
$$

```{r variance-bootstrap}
set.seed(1)

bootstrap.var <- function(sample,R){
  estimated.bootstrap <- sapply(1:R, function(idx,sample,sample.psi){
    exp.maximLikelihoodEst.lambda(rexp(length(sample),sample.psi))
  },sample,exp.maximLikelihoodEst.lambda.estimated)
  
  sum((estimated.bootstrap - mean(estimated.bootstrap))^2)/(R-1)
}

variance.bootstrap <- bootstrap.var(sample,R)
variance.bootstrap
```


`r GLOBAL.Section`## Bootstrap confidence interval

Construct a basic Bootstrap confidence interval for $\lambda$

_____

Confidence interval
$$
P(a_{\alpha} < \hat{\theta}-\theta < a_{1-\alpha})
$$


The confidence Interval can be describe like this:
$]\hat{\theta}-a_{1-\alpha},\hat{\theta}-a_{\alpha}[$

But,as we do not know the distribution of $\hat{\theta}-\theta$ we will use the empirical distribution function of $\hat{\theta^*}-\hat{\theta}$ to estimate $a_{\alpha}$ and $a_{1-\alpha}$

That being said:
$\hat{a_\alpha}=\theta_{((R+1)\alpha)}^*-\hat{\theta}$

$\hat{a_{\alpha-1}}=\theta_{((R+1)(\alpha-1))}^*-\hat{\theta}$


```{r warning=F}

set.seed(1)

boot.ci <- function(sample, R, alphaG, exp.maximLikelihoodEst.lambda.estimated, variance.bootstrap){
  alpha <- alphaG/2
  distance.booststrap <- sapply(1:R, function(idx,sample,sample.phi){
    sample.bootstrap <- rexp(length(sample),sample.phi)
    sample.bootstrap.phi <- exp.maximLikelihoodEst.lambda(sample.bootstrap)
    
    sample.bootstrap.phi - sample.phi
  },sample,exp.maximLikelihoodEst.lambda.estimated)
  
  distance.booststrap <- c(distance.booststrap,0)
  distance.booststrap.sorted <- sort(distance.booststrap, decreasing = F)
  
  ahat_left <- distance.booststrap.sorted[(R+1)*alpha]
  ahat_right <- distance.booststrap.sorted[(R+1)*(1-alpha)]
  
  sample.bootstraps.phi <- distance.booststrap+exp.maximLikelihoodEst.lambda.estimated
  ci_left <- exp.maximLikelihoodEst.lambda.estimated-ahat_right
  ci_right <- exp.maximLikelihoodEst.lambda.estimated-ahat_left
  
  return(
    list(
      sample.bootstraps.phi = sample.bootstraps.phi,
      ci_left = ci_left,
      ci_right = ci_right
    )
  )
}

boot_cis <- boot.ci(sample, R, 0.025, exp.maximLikelihoodEst.lambda.estimated, variance.bootstrap)
sample.bootstraps.phi <- boot_cis$sample.bootstraps.phi
```

```{r echo = F}
cat(sprintf("left value: %f\nright value: %f",boot_cis$ci_left,boot_cis$ci_right))
```


```{r bootstrap, warning=F, echo=F,fig.cap="Confidence intervals of 0.025 in the estimate pdf of the estimator $\\lambda$"}
confidence_interval_plot(boot_cis$sample.bootstraps.phi, boot_cis$ci_left, boot_cis$ci_right)
```

`r GLOBAL.Section`## Studentized-Bootstrap confidence interval

Construct a Studentized-Bootstrap confidence interval for $\lambda$ (Use the Bootstrap variance estimator to estimate the Bootstrap samples's variance).

_____


Studentized-Bootstrap confidence Interval can be describe like this:
$$
]\ \hat{\theta}-z_{((R+1)(1-\alpha))}^*\sqrt v \ ,\ \hat{\theta}-z_{((R+1)\alpha)}^r\sqrt v \ [
$$

The pivolal statistic used is the following:
$$
\hat{z}=\frac{\hat{\theta}-\theta}{\sqrt{var(\hat{\theta}-\theta)}}
$$
assuming that $var(\hat{\theta}-\theta)$ can be estimated by $Var_{boot}(\hat{\theta})$ then

$$
\hat{z_r^*}=\frac{\hat{\theta_r^*}-\hat{\theta}}{\sqrt{v_r^*}}
$$

This method can be more accurate when compared to the basic bootstrap interval, while being more computational demanding[@cmu-bootstrap].

```{r}
R <- 999
set.seed(1)
studentized.ci <- function(sample, R,alphaG,exp.maximLikelihoodEst.lambda.estimated,variance.bootstrap){
  alpha <- alphaG/2

  distance.booststrap.studantized <- sapply(1:R, function(idx,sample,sample.phi){
  
    sample.bootstrap <- rexp(length(sample),sample.phi)
    sample.bootstrap.phi <- exp.maximLikelihoodEst.lambda(sample.bootstrap)
    
    v <- bootstrap.var(sample.bootstrap,R)
    
    (sample.bootstrap.phi - sample.phi)/sqrt(v)
  },sample,exp.maximLikelihoodEst.lambda.estimated)
  
  distance.booststrap.studantized <- c(distance.booststrap.studantized,0)
  distance.booststrap.studantized.sorted <- sort(distance.booststrap.studantized, decreasing = F)
  
  ahat.stdzd_left <- distance.booststrap.studantized.sorted[(R+1)*alpha]*sqrt(variance.bootstrap)
  ahat.stdzd_right <- distance.booststrap.studantized.sorted[(R+1)*(1-alpha)]*sqrt(variance.bootstrap)

  
  ci_left.stdzd <- exp.maximLikelihoodEst.lambda.estimated-ahat.stdzd_right
  ci_right.stdzd <- exp.maximLikelihoodEst.lambda.estimated-ahat.stdzd_left
  return(
    list(
      ci_left = ci_left.stdzd,
      ci_right = ci_right.stdzd
    )
  )
}
studentized.ci.values <- studentized.ci(sample, R, 0.025, exp.maximLikelihoodEst.lambda.estimated, variance.bootstrap)


```



```{r echo = F, eval= F,include = F}
set.seed(1)
vs <- sapply(1:R, function(idx,sample){
  R <- 999
  sample.phi <- exp.maximLikelihoodEst.lambda(sample)
  sample.bootstrap <- rexp(length(sample),sample.phi)
  sample.bootstrap.phi <- exp.maximLikelihoodEst.lambda(sample.bootstrap)
  
  v <- bootstrap.var(sample.bootstrap,R)
  return(sqrt(v))
},sample)

distance.booststrap.studantized.df <- data.frame(
  distances = distance.booststrap.studantized
)
distance.booststrap.studantized.data <- ggplot_build(ggplot2::ggplot(distance.booststrap.studantized.df, aes(x=distances)) + geom_density())$data[[1]]

distance.booststrap.data <- ggplot_build(ggplot2::ggplot(data.frame(distance.booststrap=distance.booststrap), aes(x=distance.booststrap)) + geom_density())$data[[1]]

vs.df <- data.frame(
  vs = vs
)
vs.data <- ggplot_build(ggplot2::ggplot(vs.df, aes(x=vs)) + geom_density())$data[[1]]
```

```{r distribs-btstrp-std-html, include = F, eval=(!identical(knitr:::pandoc_to(),  'latex')), eval = F,warning=F, echo=F,fig.cap="Density estimations of pivot stat $\\hat{\\theta - \\theta}$, the bootstrap variance and the pivot stat $z$", env="figure",screenshot.opts=list(delay=1,vwidth = 1000,vheight=500), warning=F, echo=F,fig.align='center'}
plotly::subplot(
  plotly::ggplotly(ggplot2::ggplot(distance.booststrap.data,aes(x=x,y=y))+geom_line())
  ,plotly::ggplotly(ggplot2::ggplot(vs.data,aes(x=x,y=y))+geom_line())
  ,plotly::ggplotly(ggplot2::ggplot(distance.booststrap.studantized.data,aes(x=x,y=y))+geom_line())
,nrows = 3)
```

```{r distribs-btstrp-std-ltx, include = F, eval=(identical(knitr:::pandoc_to(), 'latex')), eval = F, warning=F, echo=F,fig.cap="Density estimations of pivot stat $\\hat{\\theta - \\theta}$, the bootstrap variance and the pivot stat $z$", env="figure",screenshot.opts=list(delay=1,vwidth = 1000,vheight=500), warning=F, echo=F, out.width='.90\\linewidth',fig.align='center'}
plotly::subplot(
  plotly::ggplotly(ggplot2::ggplot(distance.booststrap.data,aes(x=x,y=y))+geom_line())
  ,plotly::ggplotly(ggplot2::ggplot(vs.data,aes(x=x,y=y))+geom_line())
  ,plotly::ggplotly(ggplot2::ggplot(distance.booststrap.studantized.data,aes(x=x,y=y))+geom_line())
,nrows = 3)
```

```{r echo = F}
cat(sprintf("left value: %f\nright value: %f",studentized.ci.values$ci_left,studentized.ci.values$ci_right))
```

```{r bootstrap-stdzdz, warning=F, echo=F,fig.cap="Studentized confidence intervals of 0.025 in the estimate pdf of the estimator $\\lambda$", env="figure"}
confidence_interval_plot(sample.bootstraps.phi, studentized.ci.values$ci_left, studentized.ci.values$ci_right)
```

`r GLOBAL.Section`## Discussion

Discuss the results of items (c) and (d). Choose $\alpha = 0.025$ as the significance level for the confidence intervals.

____

The values in these confidence intervals are pretty close, with the estimated value obtain in \@ref(sub:normal-mle-var).
From [@cmu-bootstrap], the studentized-bootstrap can be more accurate when comparing to the bootstrap, but in our simulation this was not evident:

- range of bootstrap: `r round(boot_cis$ci_right - boot_cis$ci_left,3) `
- range of studentized-bootstrap: `r round(studentized.ci.values$ci_right - studentized.ci.values$ci_left,3)`

But by doing more simulations to assess this statement we can check this:

```{r eval = F}
set.seed(100)

ranges <- sapply(1:100,function(idx){
  variance.bootstrap <- bootstrap.var(sample,R)
  boot_cis <- boot.ci(sample, R, 0.025, exp.maximLikelihoodEst.lambda.estimated, variance.bootstrap)
  std_cis <- studentized.ci(sample, R, 0.025, exp.maximLikelihoodEst.lambda.estimated, variance.bootstrap)
  
  return(
    list(
      range.boot = boot_cis$ci_right - boot_cis$ci_left,
      range.boot.t = std_cis$ci_right - std_cis$ci_left
    )
  )
})

ranges.unlist <- unlist(ranges)
```

```{r include = F, eval = F}
ranges.unlist <- unlist(ranges)
ranges.unlist.df <- data.frame(
  ranges = ranges.unlist
)
write.table(ranges.unlist.df, file.path(this.path,"ranges.unlist.df"))
```

```{r include = F}
ttt <- read.table(file.path(this.path,"ranges.unlist.df"))
ranges.unlist <- ttt$ranges
```


```{r}
boot.mean <- mean(ranges.unlist[seq(1,length(ranges.unlist),2)])
boot.t.mean <- mean(ranges.unlist[seq(2,length(ranges.unlist),2)])
```

```{r include = F}
boot_boot.t_dists <- data.frame(
  boot.mean = round(boot.mean,2),
  boot.t.mean = round(boot.t.mean,2)
)
```

```{r echo=F}
kableExtra::kable(boot_boot.t_dists, knitr:::pandoc_to(), booktabs = T) %>% kableExtra::kable_styling(position = "center")
```


Given the results, we can see that, in average, the Studentized-Bootstrap can indeed be more accurate than the Boostrap, but by a small value.
This small value in difference can be because of two things:

1. Due to the small size of the sample (maybe they are not representative, which would inject a lot of uncertainty)
2. Due to the characteristics of the data. The Studentized Boostrap is not always more accurate than the Boostrap, in spite of there are cases where it is.



