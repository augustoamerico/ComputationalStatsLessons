[
["problem-1.html", "Computational Stats Deliverable 2 1 Problem 1 1.1 1 a) 1.2 1 b) 1.3 1 c)", " Computational Stats Deliverable 2 António Coelho, Gonçalo Arsénio, Sara Vigário, Tiago dos Santos 2018-11-08 1 Problem 1 Consider the following sample: c(7.0,3.5,11.9,8.9,10.1,1.2,1.1,7.9,12.9,1.3,5.2,5.1,3.9,2.5,10.4,6.2,-3.9) 1.1 1 a) Assume that the given sample originated from a random variable with a normal distribution whose parameter \\(\\sigma^2\\) is unknown. Use the sample to determine the maximum-likelihood estimator of the parameter \\(\\sigma^2\\). Maximum normal distribution Likelihood estimation. Assuming that our sample follows a normal distribution as said on the exercise question, we can say that the probability density function follows this format \\[ f_X(x_j)=(2. \\pi. \\sigma_0^2).e^{-\\frac{1}{2}.\\frac{(xj-\\mu)^2}{\\sigma_0^2}} \\] And to optain the maximum likelihood estimation, we need to partially derivate the likelihood in order to the two parameters, \\(\\mu\\) and \\(\\sigma^2\\), and discover the zeros. \\[ L(\\mu,\\sigma^2,x_1,...,x_n)=\\prod_{j=1}^n f_X(x_j;\\mu,\\sigma^2) \\] \\[ =\\prod_{j=1}^n(2\\pi\\sigma^2)^{-1/2}.e^{-\\frac{1}{2}.\\frac{(xj-\\mu)^2}{\\sigma^2}} \\] \\[ =(2\\pi\\sigma^2)^{-n/2}.e^{-\\frac{1}{2\\sigma^2}\\sum_{j=1}^{n}(xj-\\mu)^2} \\] Likelihood function: \\[ l(\\mu,\\sigma^2;x_1,...,x_n)=-\\frac{n}{2}ln(2\\pi)-\\frac{n}{2}ln(\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum_{j=1}^n(x_j-\\mu)^2 \\] Derivates equals to zero \\[ \\frac{\\partial}{\\partial \\mu}l(\\mu,\\sigma^2;x_1,...,x_n)=0 \\] \\[ \\frac{\\partial}{\\partial \\mu}l(\\mu,\\sigma^2;x_1,...,x_n)=\\frac{\\partial}{\\partial \\mu}\\bigg(-\\frac{n}{2}ln(2\\pi)-\\frac{n}{2}ln(\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum_{j=1}^n(x_j-\\mu)^2\\bigg)=\\frac{1}{\\sigma^2}(\\sum_{j=1}^nx_j-n\\mu) \\] \\[ \\frac{1}{\\sigma^2}(\\sum_{j=1}^nx_j-n\\mu)=0 \\equiv\\bigg(\\sum_{j=1}^nx_j-n\\mu\\bigg)=0 \\] Then: \\[ \\hat{\\mu}=\\frac{1}{n}\\bigg(\\sum_{j=1}^nx_j\\bigg) \\] \\[ \\frac{\\partial}{\\partial \\sigma^2}l(\\mu,\\sigma^2;x_1,...,x_n)=0 \\] \\[ \\frac{\\partial}{\\partial \\sigma^2}l(\\mu,\\sigma^2;x_1,...,x_n)=\\frac{\\partial}{\\partial \\mu}\\bigg(-\\frac{n}{2}ln(2\\pi)-\\frac{n}{2}ln(\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum_{j=1}^n(x_j-\\mu)^2)\\bigg)=\\frac{1}{2\\sigma^2}\\bigg[\\frac{1}{\\sigma^2}\\sum_{j=1}^{n}(x_j-\\mu)^2-n\\bigg] \\] \\[ \\frac{1}{2\\sigma^2}\\bigg[\\frac{1}{\\sigma^2}\\sum_{j=1}^{n}(x_j-\\mu)^2-n\\bigg]=0\\equiv\\frac{1}{\\sigma^2}\\sum_{j=1}^{n}(x_j-\\mu)^2-n=0 \\] Then: \\[ \\widehat{\\sigma^2}=\\frac{1}{n}\\sum_{j=1}^{n}(x_j-\\mu)^2 \\] sample&lt;-c(7.0,3.5,11.9,8.9,10.1,1.2,1.1,7.9,12.9,1.3,5.2,5.1,3.9,2.5,10.4,6.2,-3.9) norm.maximLikelihoodEst.var &lt;- function(sample){ n &lt;- length(sample) sample.mean &lt;- mean(sample) sum((sample - sample.mean)^2)/n } norm.maximLikelihoodEst.var(sample) ## [1] 18.84941 1.2 1 b) Determine the Jackknife bias and variance of the estimator obtained in the item (a), and discuss the quality of the estimator. We want to estimate the bias and variability of the estimator, \\(\\theta=t(F)\\). Since a good estimator not sensible to the sample is often of interest to know it faces minor pertubations in \\(F\\). We can define: \\[ L_t(y;F)=\\lim_{\\epsilon \\rightarrow 0} \\frac{t[(1-\\epsilon)F+\\epsilon H_y]-t(F)}{\\epsilon} \\] \\[ H_y= \\left\\{ \\begin{array}{ll} 1, u \\leq y \\\\ 0, u \\geq y \\end{array} \\right.\\tag{2.0} \\] as the influence fuction of \\(t\\) at \\(F\\). Using \\((2.0)\\) with the empirical function we get the empirical influence function: \\[ I(y)=L_t(y;\\hat F)\\tag{2.1} \\] Applying an extension of Taylor’s Theorem to \\(t(\\hat F)\\): \\[t(\\hat F) \\simeq t(F)+\\frac{1}{n}\\sum_{j=1}^n{I_j}\\tag{2.2}\\] So: \\[\\theta-\\hat{\\theta}=-\\frac{1}{n} \\sum_{j=1}^n{I_j}\\tag{2.3}\\] If we take \\(\\epsilon = - \\frac{1}{n-1}\\) we get a distribution with no weight on the point \\(x_j\\) and weight \\(\\frac{1}{n-1}\\) on the rest of the sample, \\(\\hat{F_{-j}}\\). In practice this is having a sample of size \\(n-1\\) by omitting \\(x_j\\) from the original sample. So, the jackknife aproximation to the empirical influence value \\(I_j\\) is: \\[I_{jack;j}=(n-1)[t(\\hat F)-t(\\hat F_{-j})]=(n-1)(\\theta-{\\hat\\theta_{-j}})\\] Consenquently: \\[b_{jack}=- \\frac{1}{n}\\sum_{j=1}^n{I_{jack;j}}\\tag{2.1}\\] \\[Var_{jack}=\\frac{1}{n(n-1)}(\\sum_{j=1}^n{I_{jack;j}^2-nb_{jack}^2)}\\tag{2.2}\\] sample&lt;-c(7.0,3.5,11.9,8.9,10.1,1.2,1.1,7.9,12.9,1.3,5.2,5.1,3.9,2.5,10.4,6.2,-3.9) n &lt;- length(sample) sample.mean &lt;- mean(sample) ljack &lt;- function(idx, sample, n){ (n-1)*(norm.maximLikelihoodEst.var(sample) - norm.maximLikelihoodEst.var(sample[-idx])) } bias.jackknife &lt;- -mean(sapply(1:n, ljack, sample, n)) bias.jackknife ## [1] -1.178088 ljack.bias.distribution &lt;- sapply(1:n, ljack, sample, n ) variance.jackknife &lt;- 1/(n*(n-1))*sum(sapply(1:n,function(idx,sample,n){ ljack(idx,sample,n)^2 - n*(bias.jackknife^2) },sample,n)) variance.jackknife ## [1] 35.37843 plot( density(-ljack.bias.distribution) , main=paste0(&quot;Density Estimation of Jackknife Bias (mean = &quot;,round(bias.jackknife,3),&quot; )&quot;) ) 1.3 1 c) Consider the null hypothesis \\(H_0\\): The sample was generated from a random variable with a normal distribution with parameters \\((\\mu, \\sigma^2)\\) = \\((5,5)\\). Use the test statistic’s empirical distribution function to estimate the Kolmogorov-Smirnov test statistic’s p-value. Explain why the test statistic is adequate considering the type of null hypothesis we are trying to test. Hypothesis testing whose hypothesis are: \\[H_0:X\\sim \\mathcal{N}(5,5) \\\\ H_1:X\\not\\sim \\mathcal{N}(5,5)\\tag{1.0}\\] Since we dont know the probability distribution function of the test statistic, \\(T(X)=D\\), we will use the empirical distribution function under the null hypothesis, \\(H_0\\), \\(\\hat F(x)_{H_0}\\), \\[\\hat F(x)_{H_0}= \\frac{\\#\\{i:x_i \\leq x\\}}{n} \\tag{1.1}\\] with \\(x_i\\) being one of \\(n\\) observations of the sample To do this, we need to use the Monte Carlo method. So, we generate \\(m\\) samples of the random variable, \\(X\\) under \\(H_0\\), \\(X\\sim \\mathcal{N}(5,5)\\), and, for each sample, compute the test statistic: \\[x_1: x_1^1, ...,x_n^1 \\rightarrow t_1=T(x^1)\\\\.\\\\.\\\\.\\\\ x_m: x_1^m, ...,x_n^m \\rightarrow t_m=T(x^m)\\tag{1.2}\\] To compare the samples with the reference probability distribution, \\(\\mathcal{N}(5,5)\\), we will use the Kolmogorov-Smirnov statistic. In short, this statistic determines the degree of similarity between two given probability distributions through distance. This distance, D, is the the largest distance between the two given cumulative distribution functions. Being \\(F(x)\\) the cumulative distribution function of the known distribution and \\(y(j)\\) the discontinuity points of \\(\\hat F(x)_{H_0}\\): \\[D=\\max_x|\\hat F(x)_{H_0}-F(x)|\\] \\[=\\max_{x}\\{\\max_x\\{\\hat F(x)_{H_0}-F(x)\\},\\max_x\\{\\hat F(x)-F(x)_{H_0}\\} \\}\\] \\[=\\max_{j=1,...,n}\\{\\frac{j}{n}-F(y(j)),F(y(j))-\\frac{j-1}{n} \\}\\tag{1.3}\\] Figure 1.1: Graph showing the distances between continuous cdf and discrete empirical cdf (Ross, page 222, 4th edition) This is well ilustrated in figure 1.1, where since both functions are monotonically increasing, a maximum distance D will only occur in the discontinuity points of \\(\\hat F(x)_{H_0}\\) or \\(y(j)\\). As previously stated, D is applied to every Monte Carlo sample. So, appling \\((1.3)\\) to \\((1.1)\\) we get: \\[ T(x^1)=D_1=\\max_{j=1,...,n}\\{\\frac{j}{n}-F(y_1(j)),F(y_1(j))-\\frac{j-1}{n}\\} \\] \\[...\\] \\[ T(x^2)=D_2=\\max_{j=1,...,n}\\{\\frac{j}{n}-F(y_2(j)),F(y_2(j))-\\frac{j-1}{n}\\} \\tag{1.4} \\] and to the original sample: \\[T(x)=d=\\max_{j=1,...,n}\\{\\frac{j}{n}-F(y(j)),F(y(j))-\\frac{j-1}{n}\\}\\tag{1.5}\\] Finally, the \\(\\hat{p-value}\\) is estimated: \\[\\hat{p-value}=P(D\\geq d|H_0) \\\\ = \\frac{\\#\\{D_k:D_k \\geq d\\}}{n} \\tag{1.6}\\] KS_statDist&lt;-function(sample,mean,sd) { sample&lt;-sort(sample) n&lt;-length(sample) DVecA&lt;-rep(0, n) DVecB&lt;-rep(0, n) for(j in c(1:n)) { DVecA[j]&lt;-(j/n)-pnorm(sample[j],mean,sd) DVecB[j]&lt;-pnorm(sample[j],mean,sd)-((j-1)/n) } DVecAA &lt;- sapply(1:n,function(idx){(j/n)-pnorm(sample[j],mean,sd)}) DVecBB &lt;- sapply(1:n,function(idx){pnorm(sample[j],mean,sd)-((j-1)/n)}) return(max(c(DVecA,DVecB))) } #Kolmogorov-Smirnov test sample.sort &lt;- sort(sample,decreasing = F) d&lt;-KS_statDist(sample.sort,5,sqrt(5)) #plot EmpCDF&lt;-ecdf(sample.sort) NormCDF&lt;-pnorm(sample.sort,5,sqrt(5)) plot(EmpCDF,main=&quot;Kolmogorov-Smirnov Test&quot;) lines(sample.sort,NormCDF,col=&quot;red&quot;) MCsampleNum&lt;-2000 m&lt;-length(sample.sort) DVec&lt;-rep(0, m) pValueNumerator&lt;-0 for (k in c(1:MCsampleNum)) { #D calculation of the generated MC samples DVec[k]&lt;-KS_statDist(rnorm(m,5,sqrt(5)),5,sqrt(5)) if(DVec[k]&gt;=d) pValueNumerator&lt;-pValueNumerator+1 } DVec.df &lt;- data.frame( pts = DVec ) library(ggplot2) p &lt;- ggplot2::ggplot(DVec.df, aes(x=pts)) + geom_density() plot(density(DVec)) #p-value calculation pValue&lt;-(pValueNumerator+1)/(MCsampleNum+1) pValue ## [1] 0.1889055 "],
["problem-2.html", "2 Problem 2 2.1 2 a) 2.2 2 b) 2.3 2 c) 2.4 2 d) 2.5 Discussion", " 2 Problem 2 2.1 2 a) Assume that the given sample originated from a random variable with a exponential distribution whose parameter \\(\\lambda\\) is unknown. Use the sample to determine the maximum-likelihood estimator of the parameter \\(\\lambda\\) Maximum normal distribution Likelihood estimation. Assuming that our sample follows a exponencial distribution as said on the exercise question, we can say that the probability density function follows this format \\[ f(x;\\lambda)=\\lambda e^{-\\lambda x} \\] And to optain the maximum likelihood estimation, we need to derivate the likelihood function in order to \\(\\lambda\\) and discover the zeros. \\[ L(\\lambda,x_1,...,x_n)=\\prod_{j=1}^n f_X(x_j;\\lambda) \\] \\[ =\\prod_{j=1}^n\\lambda e^{-\\lambda x_j} \\] \\[ =\\lambda e^{-\\lambda \\sum_{j=1}^n x_j} \\] \\[ l(\\lambda;x)= n\\ ln(\\lambda)-\\lambda(\\sum_{j=1}^n x_j) \\] \\[ \\frac{d}{d\\lambda}l(\\lambda;x)=0 \\equiv \\frac{n}{\\lambda}-\\sum_{j=1}^{n}x_j=0\\equiv\\lambda=\\frac{1}{\\frac{\\sum_{j=1}^n x_j}{n}} \\] which means \\[ \\hat{\\lambda}=\\frac{1}{\\overline{X}} \\] sample &lt;- c(0.05,0.03,0.19,0.14,0.12,0.03,0.08,0.19,0.07,0.01,0.24,0.10,0.03,0.31) exp.maximLikelihoodEst.var &lt;- function(sample){ 1/mean(sample) } exp.maximLikelihoodEst.var(sample) ## [1] 8.805031 2.2 2 b) Determine the Bootstrap bias and variance of the estimator obtained in the item (a). Viés: \\[E(\\hat{\\Psi}-\\Psi)\\approx b_{boot}(\\hat{\\Psi})= \\frac{1}{R}.\\sum_{r=1}^R.(\\hat{\\Psi}_r^*-\\hat{\\Psi})\\] R &lt;- 999 bias.bootstrap &lt;- mean(sapply(1:R, function(idx,sample){ sample.psi &lt;- exp.maximLikelihoodEst.var(sample) sample.bootstrap &lt;- rexp(length(sample),sample.psi) sample.bootstrap.psi &lt;- exp.maximLikelihoodEst.var(sample.bootstrap) sample.bootstrap.psi - sample.psi },sample)) bias.bootstrap ## [1] 0.6969684 \\[ Var(\\hat{\\Psi}-\\Psi)\\approx Var_{boot}=\\frac{1}{R-1}\\sum_{r=1}^R*(\\hat{\\Psi}_r^*-\\hat{\\Psi}^*)^2 \\] set.seed(1) bootstrap.var &lt;- function(sample,R){ estimated.bootstrap &lt;- sapply(1:R, function(idx,sample){ exp.maximLikelihoodEst.var(rexp(length(sample),exp.maximLikelihoodEst.var(sample))) },sample) sum((estimated.bootstrap - mean(estimated.bootstrap))^2)/(R-1) } variance.bootstrap &lt;- bootstrap.var(sample,R) variance.bootstrap ## [1] 8.171176 2.3 2 c) Construct a basic Bootstrap confidence interval for \\(\\lambda\\) Confidence interval \\[ P(a_{\\alpha} &lt; \\hat{\\theta}-\\theta &lt; a_{1-\\alpha}) \\] The confidence Interval can be describe like this: \\(]\\hat{\\theta}-a_{1-\\alpha},\\hat{\\theta}-a_{\\alpha}[\\) But,as we do not know the distribution of \\(\\hat{\\theta}-\\theta\\) we will use the empirical distribution function of \\(\\hat{\\theta^*}-\\hat{\\theta}\\) to estimate \\(a_{\\alpha}\\) and \\(a_{1-\\alpha}\\) That being said: \\(\\hat{a_\\alpha}=\\theta_{((R+1)\\alpha)}^*-\\hat{\\theta}\\) \\(\\hat{a_{\\alpha-1}}=\\theta_{((R+1)(\\alpha-1))}^*-\\hat{\\theta}\\) alpha &lt;- 0.025/2 set.seed(1) distance.booststrap &lt;- sapply(1:R, function(idx,sample){ sample.phi &lt;- exp.maximLikelihoodEst.var(sample) sample.bootstrap &lt;- rexp(length(sample),sample.phi) sample.bootstrap.phi &lt;- exp.maximLikelihoodEst.var(sample.bootstrap) sample.bootstrap.phi - sample.phi },sample) distance.booststrap &lt;- c(distance.booststrap,0) distance.booststrap.sorted &lt;- sort(distance.booststrap, decreasing = F) ahat_left &lt;- distance.booststrap.sorted[(R+1)*alpha] ahat_right &lt;- distance.booststrap.sorted[(R+1)*(1-alpha)] ## left value: -3.790893 ## right value: 8.600578 Figure 2.1: Density estimation of the pivot statistic \\(\\hat{\\theta - \\theta}\\) 2.4 2 d) Construct a Studentized-Bootstrap confidence interval for \\(\\lambda\\) (Use the Bootstrap variance estimator to estimate the Bootstrap samples’s variance). Studentized-Bootstrap confidence Interval can be describe like this: \\[ ]\\ \\hat{\\theta}-z_{((R+1)(1-\\alpha))}^*\\sqrt v \\ ,\\ \\hat{\\theta}-z_{((R+1)\\alpha)}^r\\sqrt v \\ [ \\] The pivolal statistic used is the following: \\[ \\hat{z}=\\frac{\\hat{\\theta}-\\theta}{\\sqrt{var(\\hat{\\theta}-\\theta)}} \\] assuming that \\(var(\\hat{\\theta}-\\theta)\\) can be estimated by \\(Var_{boot}(\\hat{\\theta})\\) then \\[ \\hat{z_r^*}=\\frac{\\hat{\\theta_r^*}-\\hat{\\theta}}{\\sqrt{v_r^*}} \\] alpha &lt;- 0.025/2 set.seed(1) vs &lt;- sapply(1:R, function(idx,sample){ R &lt;- 999 sample.phi &lt;- exp.maximLikelihoodEst.var(sample) sample.bootstrap &lt;- rexp(length(sample),sample.phi) sample.bootstrap.phi &lt;- exp.maximLikelihoodEst.var(sample.bootstrap) v &lt;- bootstrap.var(sample.bootstrap,R) return(sqrt(v)) },sample) set.seed(1) distance.booststrap.studantized &lt;- sapply(1:R, function(idx,sample){ R &lt;- 999 sample.phi &lt;- exp.maximLikelihoodEst.var(sample) sample.bootstrap &lt;- rexp(length(sample),sample.phi) sample.bootstrap.phi &lt;- exp.maximLikelihoodEst.var(sample.bootstrap) v &lt;- bootstrap.var(sample.bootstrap,R) (sample.bootstrap.phi - sample.phi)/sqrt(v) },sample) Figure 2.2: Density estimations of pivot stat \\(\\hat{\\theta - \\theta}\\), the bootstrap variance and the pivot stat \\(z\\) distance.booststrap.studantized &lt;- c(distance.booststrap.studantized,0) distance.booststrap.studantized.sorted &lt;- sort(distance.booststrap.studantized, decreasing = F) ahat.stdzd_left &lt;- distance.booststrap.studantized.sorted[(R+1)*alpha] ahat.stdzd_right &lt;- distance.booststrap.studantized.sorted[(R+1)*(1-alpha)] Figure 2.3: Density estimation of the pivot statistic \\(z\\) 2.5 Discussion Discuss the results of items (c) and (d). Choose \\(\\alpha = 0.025\\) as the significance level for the confidence intervals. The distributions in figures 2.1 and 2.3 do not present the same shape and this is due of the term \\(v_r^*\\) from \\(z_r^*=\\frac{\\hat{\\theta_r^*}-\\hat{\\theta}}{\\sqrt{v_r^*} }\\). This change of shape can also be understood in figure ?? since this occurs because the term \\(v_r^*\\) acts like a regularizator. "]
]
