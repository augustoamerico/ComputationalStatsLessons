<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Computational Stats Deliverable 2</title>
  <meta name="description" content="Computational Stats Deliverable 2">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Computational Stats Deliverable 2" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Computational Stats Deliverable 2" />
  
  
  

<meta name="author" content="António Coelho, Gonçalo Arsénio, Sara Vigário, Tiago dos Santos">


<meta name="date" content="2018-11-08">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  

<link rel="next" href="problem-2.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.3/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.8.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.39.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.39.2/plotly-latest.min.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="problem-1.html"><a href="problem-1.html"><i class="fa fa-check"></i><b>1</b> Problem 1</a><ul>
<li class="chapter" data-level="1.1" data-path="problem-1.html"><a href="problem-1.html#a"><i class="fa fa-check"></i><b>1.1</b> 1 a)</a></li>
<li class="chapter" data-level="1.2" data-path="problem-1.html"><a href="problem-1.html#b"><i class="fa fa-check"></i><b>1.2</b> 1 b)</a></li>
<li class="chapter" data-level="1.3" data-path="problem-1.html"><a href="problem-1.html#c"><i class="fa fa-check"></i><b>1.3</b> 1 c)</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="problem-2.html"><a href="problem-2.html"><i class="fa fa-check"></i><b>2</b> Problem 2</a><ul>
<li class="chapter" data-level="2.1" data-path="problem-2.html"><a href="problem-2.html#a-1"><i class="fa fa-check"></i><b>2.1</b> 2 a)</a></li>
<li class="chapter" data-level="2.2" data-path="problem-2.html"><a href="problem-2.html#b-1"><i class="fa fa-check"></i><b>2.2</b> 2 b)</a></li>
<li class="chapter" data-level="2.3" data-path="problem-2.html"><a href="problem-2.html#c-1"><i class="fa fa-check"></i><b>2.3</b> 2 c)</a></li>
<li class="chapter" data-level="2.4" data-path="problem-2.html"><a href="problem-2.html#d"><i class="fa fa-check"></i><b>2.4</b> 2 d)</a></li>
<li class="chapter" data-level="2.5" data-path="problem-2.html"><a href="problem-2.html#discussion"><i class="fa fa-check"></i><b>2.5</b> Discussion</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Computational Stats Deliverable 2</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="header">
<h1 class="title">Computational Stats Deliverable 2</h1>
<h4 class="author"><em>António Coelho, Gonçalo Arsénio, Sara Vigário, Tiago dos Santos</em></h4>
<h4 class="date"><em>2018-11-08</em></h4>
</div>
<div id="problem-1" class="section level1">
<h1><span class="header-section-number">1</span> Problem 1</h1>
<p>Consider the following sample:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">c</span>(<span class="fl">7.0</span>,<span class="fl">3.5</span>,<span class="fl">11.9</span>,<span class="fl">8.9</span>,<span class="fl">10.1</span>,<span class="fl">1.2</span>,<span class="fl">1.1</span>,<span class="fl">7.9</span>,<span class="fl">12.9</span>,<span class="fl">1.3</span>,<span class="fl">5.2</span>,<span class="fl">5.1</span>,<span class="fl">3.9</span>,<span class="fl">2.5</span>,<span class="fl">10.4</span>,<span class="fl">6.2</span>,<span class="op">-</span><span class="fl">3.9</span>)</code></pre></div>
<div id="a" class="section level2">
<h2><span class="header-section-number">1.1</span> 1 a)</h2>
<p>Assume that the given sample originated from a random variable with a normal distribution whose parameter <span class="math inline">\(\sigma^2\)</span> is unknown. Use the sample to determine the maximum-likelihood estimator of the parameter <span class="math inline">\(\sigma^2\)</span>.</p>
<hr />
<p>Maximum normal distribution Likelihood estimation.</p>
<p>Assuming that our sample follows a normal distribution as said on the exercise question, we can say that the probability density function follows this format <span class="math display">\[
f_X(x_j)=(2. \pi. \sigma_0^2).e^{-\frac{1}{2}.\frac{(xj-\mu)^2}{\sigma_0^2}}
\]</span></p>
<p>And to optain the maximum likelihood estimation, we need to partially derivate the likelihood in order to the two parameters, <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>, and discover the zeros.</p>
<p><span class="math display">\[
L(\mu,\sigma^2,x_1,...,x_n)=\prod_{j=1}^n f_X(x_j;\mu,\sigma^2)
\]</span></p>
<p><span class="math display">\[
=\prod_{j=1}^n(2\pi\sigma^2)^{-1/2}.e^{-\frac{1}{2}.\frac{(xj-\mu)^2}{\sigma^2}}
\]</span></p>
<p><span class="math display">\[
=(2\pi\sigma^2)^{-n/2}.e^{-\frac{1}{2\sigma^2}\sum_{j=1}^{n}(xj-\mu)^2}
\]</span> Likelihood function: <span class="math display">\[
l(\mu,\sigma^2;x_1,...,x_n)=-\frac{n}{2}ln(2\pi)-\frac{n}{2}ln(\sigma^2)-\frac{1}{2\sigma^2}\sum_{j=1}^n(x_j-\mu)^2
\]</span> Derivates equals to zero <span class="math display">\[
\frac{\partial}{\partial \mu}l(\mu,\sigma^2;x_1,...,x_n)=0
\]</span></p>
<p><span class="math display">\[
\frac{\partial}{\partial \mu}l(\mu,\sigma^2;x_1,...,x_n)=\frac{\partial}{\partial \mu}\bigg(-\frac{n}{2}ln(2\pi)-\frac{n}{2}ln(\sigma^2)-\frac{1}{2\sigma^2}\sum_{j=1}^n(x_j-\mu)^2\bigg)=\frac{1}{\sigma^2}(\sum_{j=1}^nx_j-n\mu)
\]</span></p>
<p><span class="math display">\[
\frac{1}{\sigma^2}(\sum_{j=1}^nx_j-n\mu)=0 \equiv\bigg(\sum_{j=1}^nx_j-n\mu\bigg)=0
\]</span> Then: <span class="math display">\[
\hat{\mu}=\frac{1}{n}\bigg(\sum_{j=1}^nx_j\bigg)
\]</span></p>
<p><span class="math display">\[
\frac{\partial}{\partial \sigma^2}l(\mu,\sigma^2;x_1,...,x_n)=0
\]</span> <span class="math display">\[
\frac{\partial}{\partial \sigma^2}l(\mu,\sigma^2;x_1,...,x_n)=\frac{\partial}{\partial \mu}\bigg(-\frac{n}{2}ln(2\pi)-\frac{n}{2}ln(\sigma^2)-\frac{1}{2\sigma^2}\sum_{j=1}^n(x_j-\mu)^2)\bigg)=\frac{1}{2\sigma^2}\bigg[\frac{1}{\sigma^2}\sum_{j=1}^{n}(x_j-\mu)^2-n\bigg]
\]</span> <span class="math display">\[
\frac{1}{2\sigma^2}\bigg[\frac{1}{\sigma^2}\sum_{j=1}^{n}(x_j-\mu)^2-n\bigg]=0\equiv\frac{1}{\sigma^2}\sum_{j=1}^{n}(x_j-\mu)^2-n=0
\]</span> Then: <span class="math display">\[
\widehat{\sigma^2}=\frac{1}{n}\sum_{j=1}^{n}(x_j-\mu)^2
\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sample&lt;-<span class="kw">c</span>(<span class="fl">7.0</span>,<span class="fl">3.5</span>,<span class="fl">11.9</span>,<span class="fl">8.9</span>,<span class="fl">10.1</span>,<span class="fl">1.2</span>,<span class="fl">1.1</span>,<span class="fl">7.9</span>,<span class="fl">12.9</span>,<span class="fl">1.3</span>,<span class="fl">5.2</span>,<span class="fl">5.1</span>,<span class="fl">3.9</span>,<span class="fl">2.5</span>,<span class="fl">10.4</span>,<span class="fl">6.2</span>,<span class="op">-</span><span class="fl">3.9</span>)

norm.maximLikelihoodEst.var &lt;-<span class="st"> </span><span class="cf">function</span>(sample){
  n &lt;-<span class="st"> </span><span class="kw">length</span>(sample)
  sample.mean &lt;-<span class="st"> </span><span class="kw">mean</span>(sample)
  <span class="kw">sum</span>((sample <span class="op">-</span><span class="st"> </span>sample.mean)<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span>n
}

<span class="kw">norm.maximLikelihoodEst.var</span>(sample)</code></pre></div>
<pre><code>## [1] 18.84941</code></pre>
</div>
<div id="b" class="section level2">
<h2><span class="header-section-number">1.2</span> 1 b)</h2>
<p>Determine the Jackknife bias and variance of the estimator obtained in the item (a), and discuss the quality of the estimator.</p>
<hr />
<p>We want to estimate the bias and variability of the estimator, <span class="math inline">\(\theta=t(F)\)</span>. Since a good estimator not sensible to the sample is often of interest to know it faces minor pertubations in <span class="math inline">\(F\)</span>. We can define:</p>
<p><span class="math display">\[
L_t(y;F)=\lim_{\epsilon \rightarrow 0} \frac{t[(1-\epsilon)F+\epsilon H_y]-t(F)}{\epsilon}
\]</span> <span class="math display">\[
  H_y= \left\{
  \begin{array}{ll}
  1, u \leq y
  \\
  0, u \geq y
  \end{array}
  \right.\tag{2.0}
\]</span></p>
<p>as the influence fuction of <span class="math inline">\(t\)</span> at <span class="math inline">\(F\)</span>.</p>
<p>Using <span class="math inline">\((2.0)\)</span> with the empirical function we get the empirical influence function:</p>
<p><span class="math display">\[
  I(y)=L_t(y;\hat F)\tag{2.1}
\]</span> Applying an extension of Taylor’s Theorem to <span class="math inline">\(t(\hat F)\)</span>:</p>
<p><span class="math display">\[t(\hat F) \simeq t(F)+\frac{1}{n}\sum_{j=1}^n{I_j}\tag{2.2}\]</span> So:</p>
<p><span class="math display">\[\theta-\hat{\theta}=-\frac{1}{n} \sum_{j=1}^n{I_j}\tag{2.3}\]</span></p>
<p>If we take <span class="math inline">\(\epsilon = - \frac{1}{n-1}\)</span> we get a distribution with no weight on the point <span class="math inline">\(x_j\)</span> and weight <span class="math inline">\(\frac{1}{n-1}\)</span> on the rest of the sample, <span class="math inline">\(\hat{F_{-j}}\)</span>. In practice this is having a sample of size <span class="math inline">\(n-1\)</span> by omitting <span class="math inline">\(x_j\)</span> from the original sample.</p>
<p>So, the jackknife aproximation to the empirical influence value <span class="math inline">\(I_j\)</span> is:</p>
<p><span class="math display">\[I_{jack;j}=(n-1)[t(\hat F)-t(\hat F_{-j})]=(n-1)(\theta-{\hat\theta_{-j}})\]</span></p>
<p>Consenquently:</p>
<p><span class="math display">\[b_{jack}=- \frac{1}{n}\sum_{j=1}^n{I_{jack;j}}\tag{2.1}\]</span> <span class="math display">\[Var_{jack}=\frac{1}{n(n-1)}(\sum_{j=1}^n{I_{jack;j}^2-nb_{jack}^2)}\tag{2.2}\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sample&lt;-<span class="kw">c</span>(<span class="fl">7.0</span>,<span class="fl">3.5</span>,<span class="fl">11.9</span>,<span class="fl">8.9</span>,<span class="fl">10.1</span>,<span class="fl">1.2</span>,<span class="fl">1.1</span>,<span class="fl">7.9</span>,<span class="fl">12.9</span>,<span class="fl">1.3</span>,<span class="fl">5.2</span>,<span class="fl">5.1</span>,<span class="fl">3.9</span>,<span class="fl">2.5</span>,<span class="fl">10.4</span>,<span class="fl">6.2</span>,<span class="op">-</span><span class="fl">3.9</span>)
n &lt;-<span class="st"> </span><span class="kw">length</span>(sample)
sample.mean &lt;-<span class="st"> </span><span class="kw">mean</span>(sample)


ljack &lt;-<span class="st"> </span><span class="cf">function</span>(idx, sample, n){
  (n<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>(<span class="kw">norm.maximLikelihoodEst.var</span>(sample) <span class="op">-</span><span class="st"> </span><span class="kw">norm.maximLikelihoodEst.var</span>(sample[<span class="op">-</span>idx]))
}

bias.jackknife &lt;-<span class="st"> </span><span class="op">-</span><span class="kw">mean</span>(<span class="kw">sapply</span>(<span class="dv">1</span><span class="op">:</span>n, ljack, sample, n))
bias.jackknife</code></pre></div>
<pre><code>## [1] -1.178088</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ljack.bias.distribution &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="dv">1</span><span class="op">:</span>n, ljack, sample, n )

variance.jackknife &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">/</span>(n<span class="op">*</span>(n<span class="op">-</span><span class="dv">1</span>))<span class="op">*</span><span class="kw">sum</span>(<span class="kw">sapply</span>(<span class="dv">1</span><span class="op">:</span>n,<span class="cf">function</span>(idx,sample,n){
  <span class="kw">ljack</span>(idx,sample,n)<span class="op">^</span><span class="dv">2</span> <span class="op">-</span><span class="st"> </span>n<span class="op">*</span>(bias.jackknife<span class="op">^</span><span class="dv">2</span>)
},sample,n))
variance.jackknife</code></pre></div>
<pre><code>## [1] 35.37843</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(
  <span class="kw">density</span>(<span class="op">-</span>ljack.bias.distribution)
  , <span class="dt">main=</span><span class="kw">paste0</span>(<span class="st">&quot;Density Estimation of Jackknife Bias (mean = &quot;</span>,<span class="kw">round</span>(bias.jackknife,<span class="dv">3</span>),<span class="st">&quot; )&quot;</span>)
)</code></pre></div>
<p><img src="Computational_Statistics_Deliverable_2_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
</div>
<div id="c" class="section level2">
<h2><span class="header-section-number">1.3</span> 1 c)</h2>
<p>Consider the null hypothesis <span class="math inline">\(H_0\)</span>: The sample was generated from a random variable with a normal distribution with parameters <span class="math inline">\((\mu, \sigma^2)\)</span> = <span class="math inline">\((5,5)\)</span>. Use the test statistic’s empirical distribution function to estimate the Kolmogorov-Smirnov test statistic’s p-value. Explain why the test statistic is adequate considering the type of null hypothesis we are trying to test.</p>
<hr />
<p>Hypothesis testing whose hypothesis are:</p>
<p><span class="math display">\[H_0:X\sim \mathcal{N}(5,5) \\ H_1:X\not\sim \mathcal{N}(5,5)\tag{1.0}\]</span></p>
<p>Since we dont know the probability distribution function of the test statistic, <span class="math inline">\(T(X)=D\)</span>, we will use the empirical distribution function under the null hypothesis, <span class="math inline">\(H_0\)</span>, <span class="math inline">\(\hat F(x)_{H_0}\)</span>,</p>
<p><span class="math display">\[\hat F(x)_{H_0}= \frac{\#\{i:x_i \leq x\}}{n} \tag{1.1}\]</span></p>
<p>with <span class="math inline">\(x_i\)</span> being one of <span class="math inline">\(n\)</span> observations of the sample</p>
<p>To do this, we need to use the Monte Carlo method.</p>
<p>So, we generate <span class="math inline">\(m\)</span> samples of the random variable, <span class="math inline">\(X\)</span> under <span class="math inline">\(H_0\)</span>, <span class="math inline">\(X\sim \mathcal{N}(5,5)\)</span>, and, for each sample, compute the test statistic:</p>
<p><span class="math display">\[x_1: x_1^1, ...,x_n^1 \rightarrow t_1=T(x^1)\\.\\.\\.\\ x_m: x_1^m, ...,x_n^m \rightarrow t_m=T(x^m)\tag{1.2}\]</span></p>
<p>To compare the samples with the reference probability distribution, <span class="math inline">\(\mathcal{N}(5,5)\)</span>, we will use the Kolmogorov-Smirnov statistic.</p>
<p>In short, this statistic determines the degree of similarity between two given probability distributions through distance. This distance, D, is the the largest distance between the two given cumulative distribution functions.</p>
<p>Being <span class="math inline">\(F(x)\)</span> the cumulative distribution function of the known distribution and <span class="math inline">\(y(j)\)</span> the discontinuity points of <span class="math inline">\(\hat F(x)_{H_0}\)</span>:</p>
<p><span class="math display">\[D=\max_x|\hat F(x)_{H_0}-F(x)|\]</span> <span class="math display">\[=\max_{x}\{\max_x\{\hat F(x)_{H_0}-F(x)\},\max_x\{\hat F(x)-F(x)_{H_0}\} \}\]</span> <span class="math display">\[=\max_{j=1,...,n}\{\frac{j}{n}-F(y(j)),F(y(j))-\frac{j-1}{n} \}\tag{1.3}\]</span></p>
<div class="figure"><span id="fig:ross-ks"></span>
<img src="assets/ross_ks_test.png" alt="Graph showing the distances between continuous cdf and discrete empirical cdf (Ross, page 222, 4th edition)"  />
<p class="caption">
Figure 1.1: Graph showing the distances between continuous cdf and discrete empirical cdf (Ross, page 222, 4th edition)
</p>
</div>
<p>This is well ilustrated in figure <a href="problem-1.html#fig:ross-ks">1.1</a>, where since both functions are monotonically increasing, a maximum distance D will only occur in the discontinuity points of <span class="math inline">\(\hat F(x)_{H_0}\)</span> or <span class="math inline">\(y(j)\)</span>.</p>
<p>As previously stated, D is applied to every Monte Carlo sample. So, appling <span class="math inline">\((1.3)\)</span> to <span class="math inline">\((1.1)\)</span> we get:</p>
<p><span class="math display">\[
T(x^1)=D_1=\max_{j=1,...,n}\{\frac{j}{n}-F(y_1(j)),F(y_1(j))-\frac{j-1}{n}\}
\]</span> <span class="math display">\[...\]</span> <span class="math display">\[
  T(x^2)=D_2=\max_{j=1,...,n}\{\frac{j}{n}-F(y_2(j)),F(y_2(j))-\frac{j-1}{n}\} \tag{1.4}
\]</span> and to the original sample:</p>
<p><span class="math display">\[T(x)=d=\max_{j=1,...,n}\{\frac{j}{n}-F(y(j)),F(y(j))-\frac{j-1}{n}\}\tag{1.5}\]</span></p>
<p>Finally, the <span class="math inline">\(\hat{p-value}\)</span> is estimated:</p>
<p><span class="math display">\[\hat{p-value}=P(D\geq d|H_0) \\ = \frac{\#\{D_k:D_k \geq d\}}{n} \tag{1.6}\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">KS_statDist&lt;-<span class="cf">function</span>(sample,mean,sd)
{
  sample&lt;-<span class="kw">sort</span>(sample)
  n&lt;-<span class="kw">length</span>(sample)
  DVecA&lt;-<span class="kw">rep</span>(<span class="dv">0</span>, n)
  DVecB&lt;-<span class="kw">rep</span>(<span class="dv">0</span>, n)
  <span class="cf">for</span>(j <span class="cf">in</span> <span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span>n))
  {
    DVecA[j]&lt;-(j<span class="op">/</span>n)<span class="op">-</span><span class="kw">pnorm</span>(sample[j],mean,sd)
    DVecB[j]&lt;-<span class="kw">pnorm</span>(sample[j],mean,sd)<span class="op">-</span>((j<span class="op">-</span><span class="dv">1</span>)<span class="op">/</span>n)
  }
  DVecAA &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="dv">1</span><span class="op">:</span>n,<span class="cf">function</span>(idx){(j<span class="op">/</span>n)<span class="op">-</span><span class="kw">pnorm</span>(sample[j],mean,sd)})
  DVecBB &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="dv">1</span><span class="op">:</span>n,<span class="cf">function</span>(idx){<span class="kw">pnorm</span>(sample[j],mean,sd)<span class="op">-</span>((j<span class="op">-</span><span class="dv">1</span>)<span class="op">/</span>n)})
  <span class="kw">return</span>(<span class="kw">max</span>(<span class="kw">c</span>(DVecA,DVecB)))
}

<span class="co">#Kolmogorov-Smirnov test</span>
sample.sort &lt;-<span class="st"> </span><span class="kw">sort</span>(sample,<span class="dt">decreasing =</span> F)
d&lt;-<span class="kw">KS_statDist</span>(sample.sort,<span class="dv">5</span>,<span class="kw">sqrt</span>(<span class="dv">5</span>))

<span class="co">#plot</span>
EmpCDF&lt;-<span class="kw">ecdf</span>(sample.sort)
NormCDF&lt;-<span class="kw">pnorm</span>(sample.sort,<span class="dv">5</span>,<span class="kw">sqrt</span>(<span class="dv">5</span>))
<span class="kw">plot</span>(EmpCDF,<span class="dt">main=</span><span class="st">&quot;Kolmogorov-Smirnov Test&quot;</span>)
<span class="kw">lines</span>(sample.sort,NormCDF,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</code></pre></div>
<p><img src="Computational_Statistics_Deliverable_2_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">MCsampleNum&lt;-<span class="dv">2000</span>
m&lt;-<span class="kw">length</span>(sample.sort)
DVec&lt;-<span class="kw">rep</span>(<span class="dv">0</span>, m)
pValueNumerator&lt;-<span class="dv">0</span>
<span class="cf">for</span> (k <span class="cf">in</span> <span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span>MCsampleNum)) 
{
  <span class="co">#D calculation of the generated MC samples </span>
  DVec[k]&lt;-<span class="kw">KS_statDist</span>(<span class="kw">rnorm</span>(m,<span class="dv">5</span>,<span class="kw">sqrt</span>(<span class="dv">5</span>)),<span class="dv">5</span>,<span class="kw">sqrt</span>(<span class="dv">5</span>))
  <span class="cf">if</span>(DVec[k]<span class="op">&gt;=</span>d)
    pValueNumerator&lt;-pValueNumerator<span class="op">+</span><span class="dv">1</span>
}

DVec.df &lt;-<span class="st"> </span><span class="kw">data.frame</span>(
  <span class="dt">pts =</span> DVec
)
<span class="kw">library</span>(ggplot2)
p &lt;-<span class="st"> </span>ggplot2<span class="op">::</span><span class="kw">ggplot</span>(DVec.df, <span class="kw">aes</span>(<span class="dt">x=</span>pts)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_density</span>()
<span class="kw">plot</span>(<span class="kw">density</span>(DVec))</code></pre></div>
<p><img src="Computational_Statistics_Deliverable_2_files/figure-html/unnamed-chunk-7-2.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#p-value calculation</span>
pValue&lt;-(pValueNumerator<span class="op">+</span><span class="dv">1</span>)<span class="op">/</span>(MCsampleNum<span class="op">+</span><span class="dv">1</span>)
pValue</code></pre></div>
<pre><code>## [1] 0.1889055</code></pre>
</div>
</div>
            </section>

          </div>
        </div>
      </div>

<a href="problem-2.html" class="navigation navigation-next navigation-unique" aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
